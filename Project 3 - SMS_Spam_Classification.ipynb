{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam Classification - Feature Engineering, Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"spamdata.csv\",encoding = \"latin-1\")\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work on Feature Engineering, first we need to work on Text Preprocessing to get rid of Noise or remove unnecessary/reduntant information present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play game today'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def _clean(text):\n",
    "    \n",
    "    cleaned_text = text.lower()\n",
    "    \n",
    "    cleaned_text = \"\".join(c for c in cleaned_text if c not in punctuations)\n",
    "    \n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    words = [w for w in words if w not in stopword_list]\n",
    "    \n",
    "    words = [lem.lemmatize(word,\"v\") for word in words] # Stemming doesn't need any POS Tagging but Lemmatization requrires\n",
    "    words = [lem.lemmatize(word,\"n\") for word in words] # Generally Verbs and Nouns are the most important\n",
    "    \n",
    "    cleaned_text = \" \".join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "_clean(\"I will by playing a game today !! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                              ok lar joke wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf live around though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can the same function to run on all the dataset\n",
    "\n",
    "data[\"cleaned\"] = data[\"text\"].apply(_clean)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is preprocessed data. We are yet to perform text cleaning techniques.\n",
    "\n",
    "# We didn't remove other support or domain specific words, didn't corrected spellings such as \"tkts\" instead of \"tickets\"\n",
    "\n",
    "# This can be improved further according to the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature Engineering\n",
    "\n",
    "# We need to generate different types of feature to convert the text data to useful for ML Algorithms. Examples are below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta features - Count of attributes associated with the text data\n",
    "\n",
    "data[\"word_count\"] = data[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "data[\"word_count_cleaned\"] = data[\"cleaned\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "data[\"char_count\"] = data[\"text\"].apply(lambda x: len(x)) # Total number of characters in the text\n",
    "\n",
    "data[\"char_count_without_spaces\"] = data[\"text\"].apply(lambda x: len(x.replace(\" \",\"\"))) # Getting rid of spaces\n",
    "\n",
    "# very important for SPAM as they always say free ticket number\n",
    "\n",
    "data[\"num_dig\"]= data[\"text\"].apply(lambda x: sum([1 if w.isdigit() else 0 for w in x.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_cleaned</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_without_spaces</th>\n",
       "      <th>num_dig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  word_count  \\\n",
       "0  go jurong point crazy available bugis n great ...          20   \n",
       "1                              ok lar joke wif u oni           6   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...          28   \n",
       "3                u dun say early hor u c already say          11   \n",
       "4           nah dont think go usf live around though          13   \n",
       "\n",
       "   word_count_cleaned  char_count  char_count_without_spaces  num_dig  \n",
       "0                  16         111                         92        0  \n",
       "1                   6          29                         24        0  \n",
       "2                  23         155                        128        2  \n",
       "3                   9          49                         39        0  \n",
       "4                   8          61                         49        0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the meta features of these dataframe. These features can be used in regression, classification, recommendation engines et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Based features as every text is associated with NLP Properties lik3\n",
    "\n",
    "# Dependency grammar, relationship among other words, POS Tags\n",
    "\n",
    "# E.g. Noun represents the kind of entities, subjects or objects whereas Verb represents the actions or action keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To generate POS Tags features, we can first create POS Dictionary\n",
    "\n",
    "# NNP - proper noun, NN - Regular Nouns, NNS - Singular Nouns\n",
    "# Verbs for noun families are adding\n",
    "\n",
    "pos_dic = {\"noun\":[\"NNP\",\"NN\",\"NNS\",\"NNPS\"], \"verb\":[\"VBZ\",\"VB\",\"VBD\",\"VBN\",\"VBG\"]} \n",
    "import nltk                       \n",
    "\n",
    "def pos_check(txt,family):\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(txt))\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        tag = tag[1] #to obtain the proper POS\n",
    "        if tag in pos_dic[family]:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "pos_check(\"They are playing in the ground\",\"noun\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one noun is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_check(\"They are playing in the ground\",\"verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"noun_count\"] = data[\"text\"].apply(lambda x : pos_check(x,\"noun\"))\n",
    "data[\"verb_count\"] = data[\"text\"].apply(lambda x : pos_check(x,\"verb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_cleaned</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_without_spaces</th>\n",
       "      <th>num_dig</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  word_count  \\\n",
       "0  go jurong point crazy available bugis n great ...          20   \n",
       "1                              ok lar joke wif u oni           6   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...          28   \n",
       "3                u dun say early hor u c already say          11   \n",
       "4           nah dont think go usf live around though          13   \n",
       "\n",
       "   word_count_cleaned  char_count  char_count_without_spaces  num_dig  \\\n",
       "0                  16         111                         92        0   \n",
       "1                   6          29                         24        0   \n",
       "2                  23         155                        128        2   \n",
       "3                   9          49                         39        0   \n",
       "4                   8          61                         49        0   \n",
       "\n",
       "   noun_count  verb_count  \n",
       "0          10           1  \n",
       "1           4           0  \n",
       "2          13           3  \n",
       "3           3           0  \n",
       "4           1           4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apart from these Meta Feature, NLP Based Features, POS Tag Features....we can also work on vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering\n",
    "\n",
    "# One of the idea to create Word vectors is count as the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countvectorizer is a class which can be used to generate count vectors for all the particular documents present in the corpus\n",
    "\n",
    "# Similar to Countvectorizer, Tfidfvectorizer generates term frequency and inverse document frequency and their corresponding\n",
    "\n",
    "# product as the elements of the word vectors\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cvz = CountVectorizer()\n",
    "cvz.fit(data[\"cleaned\"].values) # we are fitting to cleaned data as we don't unnecessary noise\n",
    "\n",
    "count_vectors = cvz.transform(data[\"cleaned\"].values) # this transforms original data into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8206 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 46827 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors # This matrix will contain all word vectors corresponding to their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Tfidf \n",
    "\n",
    "word_tfidf = TfidfVectorizer()\n",
    "word_tfidf.fit(data[\"cleaned\"].values)\n",
    "word_vectors_tfidf = word_tfidf.transform(data[\"cleaned\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8206 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46827 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes Tf-idf for every word present in the cleaned data for us and in the end, they replace their corresponding tfidf\n",
    "\n",
    "# score in the location where their words are present.\n",
    "\n",
    "word_vectors_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>008704050406</td>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0089my</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0121</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01223585236</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01223585334</td>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>¹ã</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>âªm</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>âªt</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>âªve</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>â¼120</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "008704050406  8.527076\n",
       "0089my        8.932542\n",
       "0121          8.932542\n",
       "01223585236   8.932542\n",
       "01223585334   8.527076\n",
       "...                ...\n",
       "¹ã            8.932542\n",
       "âªm           8.932542\n",
       "âªt           8.932542\n",
       "âªve          8.932542\n",
       "â¼120         8.932542\n",
       "\n",
       "[8206 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = dict(zip(word_tfidf.get_feature_names(),word_tfidf.idf_))\n",
    "\n",
    "pd.DataFrame(columns=[\"word_tfidf\"]).from_dict(tfidf, orient = \"index\")\n",
    "\n",
    "# This gives all the keywords and corresponding Tfidf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>008704050406</td>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0089my</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0121</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01223585236</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01223585334</td>\n",
       "      <td>8.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>¹ã</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>âªm</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>âªt</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>âªve</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>â¼120</td>\n",
       "      <td>8.932542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word_tfidf\n",
       "008704050406    8.527076\n",
       "0089my          8.932542\n",
       "0121            8.932542\n",
       "01223585236     8.932542\n",
       "01223585334     8.527076\n",
       "...                  ...\n",
       "¹ã              8.932542\n",
       "âªm             8.932542\n",
       "âªt             8.932542\n",
       "âªve            8.932542\n",
       "â¼120           8.932542\n",
       "\n",
       "[8206 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_idf = pd.DataFrame(columns=[\"word_tfidf\"]).from_dict(tfidf, orient = \"index\")\n",
    "\n",
    "tfidf_idf.columns =[\"word_tfidf\"]\n",
    "\n",
    "tfidf_idf # Where so ever these words will be present, they will be now represented by tfidf score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have created so many features like meta features, NLP based features, raw counts like noun, verb family, frequency based etc. now to use them we need to combine them as a matrix which will be a Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'cleaned', 'word_count', 'word_count_cleaned',\n",
       "       'char_count', 'char_count_without_spaces', 'num_dig', 'noun_count',\n",
       "       'verb_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8213 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 80199 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will stack all our features into horizontally hence hstack\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "meta_features = ['word_count', 'word_count_cleaned',\n",
    "       'char_count', 'char_count_without_spaces', 'num_dig', 'noun_count',\n",
    "       'verb_count']\n",
    "\n",
    "features_set1 = data[meta_features]\n",
    "\n",
    "train = hstack([word_vectors_tfidf, csr_matrix(features_set1)],\"csr\")\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5572 rows X 8213 columns \n",
    "\n",
    "These many columns is due to as it is obtained from tfidf and some of the stacked columns of the features_set1. Notice that there are less number rows and more number of columns. This is because in tfidf we generated so many tfidf features. \n",
    "\n",
    "\n",
    "Inside the tfidf, we can also control the number of features like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tfidf = TfidfVectorizer(max_features = 500)\n",
    "word_tfidf.fit(data[\"cleaned\"].values)\n",
    "word_vectors_tfidf = word_tfidf.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28313 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_tfidf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see only 500 Features are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x507 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 61685 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = dict(zip(word_tfidf.get_feature_names(),word_tfidf.idf_))\n",
    "\n",
    "pd.DataFrame(columns=[\"word_tfidf\"]).from_dict(tfidf, orient = \"index\")\n",
    "\n",
    "\n",
    "tfidf_idf = pd.DataFrame(columns=[\"word_tfidf\"]).from_dict(tfidf, orient = \"index\")\n",
    "\n",
    "tfidf_idf.columns =[\"word_tfidf\"]\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "meta_features = ['word_count', 'word_count_cleaned',\n",
    "       'char_count', 'char_count_without_spaces', 'num_dig', 'noun_count',\n",
    "       'verb_count']\n",
    "\n",
    "features_set1 = data[meta_features]\n",
    "\n",
    "train = hstack([word_vectors_tfidf, csr_matrix(features_set1)],\"csr\")\n",
    "\n",
    "train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 Columns are tfidf featues and 7 correspoonds to meata features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used word vectors for tfidf, there are some variations we can also do in word level tfidf. Instead of word, we can use n-gram level tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 corresponds to unigram and 2 corresponds to bigrams\n",
    "\n",
    "# when we specify 1,4 it will generate all the possible phrases between 1 and 4\n",
    "\n",
    "ngram_tfidf = TfidfVectorizer(max_features = 500, ngram_range = (1,2)) \n",
    "ngram_tfidf.fit(data[\"cleaned\"].values)\n",
    "ngram_vectors_tfidf = ngram_tfidf.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of stacking word level tfidf, we can also do n-gram level tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variation can be character level tfidf. Instead of counting word, this counts each and every character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tfidf = TfidfVectorizer(max_features = 500,analyzer = \"char\")\n",
    "char_tfidf.fit(data[\"cleaned\"].values)\n",
    "char_vectors_tfidf = char_tfidf.transform(data[\"cleaned\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives character counts and corresponding tfidf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While stacking we have a choice of using either only word level, ngram or character or we can use both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with word_count, we can generate upper case, lower case count, special case count, punctuation count. In POS tag family, we can use adjective count, adverb count etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, train matrix is our final input which can be given to ML Algorithms, recommendations and search engines etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x507 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 61685 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is about Feature Engineering. We discussed preprocessing on a given dataset and different featurs can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the generated features in ML Algo to classify whether given text is Spam or Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x507 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 61685 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a matrix where we horizontally stacked on the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Label encode our target variable which is Spam or Ham but ML Models need numbers to work with. We can convert the categorical variable into label encoded(0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "target = data[\"label\"].values\n",
    "\n",
    "target = LabelEncoder().fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target # this is an array where 0 corresponds to Ham and 1 corresponds to Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, val_x,train_y, val_y = train_test_split(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 507)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393, 507)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968413496051687"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = naive_bayes.MultinomialNB()\n",
    "model.fit(train_x,train_y)\n",
    "preds = model.predict(val_x)\n",
    "accuracy_score(preds,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chalampp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9597989949748744"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_x,train_y)\n",
    "preds = model.predict(val_x)\n",
    "accuracy_score(preds,val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost similar accuracy, this is because the dataset has less number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9267767408470926"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(train_x,train_y)\n",
    "preds = model.predict(val_x)\n",
    "accuracy_score(preds,val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is not a good choice here. SVM works well for text classification when Tfidf scores are good with good amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9806173725771715"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ensemble.ExtraTreesClassifier()\n",
    "model.fit(train_x,train_y)\n",
    "preds = model.predict(val_x)\n",
    "accuracy_score(preds,val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in this problem is don't need to go for a complex model because simple models are able to give good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add more amount of data, this model show variations. Hence bagging (random forests) or boosting models (XGboos, Lightgbm, catboost) gives good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically CNN for text classifications purposes. To perform CNN on our text data we first need to represent our text inputs in word embedding format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea to use the word embedding format is either pretrained word models provided by the libraries such as word2vec, Glove, fasttext etc. or train the models and their corresponding word embeddings from scratch. The idea from scratch works well when we have huge amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have only limited (5500 rows) hence let us pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 , -0.0282 -0.0557 -0.0451 -0.0434 0.0712 -0.0855 -0.1085 -0.0561 -0.4523 -0.0202 0.0975 0.1047 0.1962 -0.0693 0.0213 -0.0235 0.1336 -0.0420 -0.0564 -0.0798 0.0424 -0.0409 -0.0536 -0.0252 0.0135 0.0064 0.1235 0.0461 0.0120 -0.0372 0.0650 0.0041 -0.1074 -0.0263 0.1133 -0.0029 0.0671 0.1065 0.0234 -0.0160 0.0070 0.4355 -0.0752 -0.4328 0.0457 0.0604 -0.0740 -0.0055 -0.0089 -0.2926 -0.0545 -0.1519 0.0990 -0.0193 -0.0050 0.0511 0.0404 0.1023 -0.0128 0.0488 -0.1567 -0.0759 -0.0190 0.1442 0.0047 -0.0186 0.0140 -0.0385 -0.0853 0.1572 0.1770 0.0084 -0.0250 -0.1145 -0.0663 -0.1244 -0.3977 -0.0124 -0.4586 -0.0220 0.5746 0.0218 -0.0754 0.0099 0.0397 -0.0154 0.0424 -0.0150 -0.0016 0.0305 0.0101 0.2266 0.1394 0.0189 0.0069 0.0394 0.0355 -0.0111 -0.0687 -0.0078 0.0224 0.0817 -0.1949 0.0001 0.4047 -0.0237 -0.0656 -0.0684 0.0233 0.0438 0.1203 -0.0276 0.0416 0.0114 -0.4529 0.1538 0.1323 -0.0186 -0.0914 -0.0312 0.1051 0.0212 0.0798 -0.0104 -0.0206 -0.0025 0.0043 -0.0378 0.2689 0.0747 -0.0418 -0.0048 -0.0387 0.0432 0.1704 0.0614 0.0905 -0.0436 -0.0141 -0.0315 0.0276 0.0151 -0.0103 -0.0266 -0.0512 -0.0408 -0.0651 0.0662 -0.0936 0.1371 0.0458 -0.1366 -0.0075 -0.0104 -0.0732 0.1205 0.1035 0.0106 -0.0317 -0.0316 0.6639 -0.0022 -0.1343 0.0144 -0.0338 0.0034 -0.0429 -0.0821 0.0037 0.1029 -0.0204 -0.0269 0.0052 -0.1034 0.1068 0.0121 0.0980 -0.0458 0.0199 -0.0132 0.1936 -0.0213 0.0209 -0.0025 0.0416 -0.0337 0.0516 -0.1014 0.0203 0.0198 -0.0305 -0.0313 0.0543 -0.0106 0.1441 -0.0178 -0.0627 0.0475 0.0352 -0.0254 -0.0949 0.0401 0.0317 0.0055 -0.0536 0.0191 -0.0511 -0.0409 -0.0030 0.1582 0.0108 0.5237 0.0436 0.0306 -0.0392 0.0177 0.0069 0.0605 0.1206 -0.0216 -0.0633 -0.2965 0.0521 -0.0150 -0.2207 -0.0642 -0.0906 -0.0121 0.0569 0.0944 -0.0652 -0.0108 -0.0477 0.0023 0.0077 -0.1547 0.0463 0.0698 -0.0376 -0.0291 0.0033 -0.0102 -0.0743 0.0085 0.0805 -0.0291 -0.0674 -0.0586 -0.0653 0.0283 -0.0255 0.0869 -0.0868 0.0090 0.3245 -0.0573 -0.0289 0.0470 -0.0117 0.0174 0.0132 -0.0226 -0.0664 0.0188 0.0263 0.0111 -0.0049 -0.0656 0.0295 0.0435 0.0290 0.1163 0.0448 -0.1139 -0.0553 -0.0528 0.1745 -0.0146 -0.1308 -0.0607 -0.0134 0.0781 0.0378 0.0228 -0.0728 -0.0059 0.0158 -0.0141 -0.0002 0.0193 -0.0148 -0.0463 0.0444 0.3034 0.1020 -0.0871 0.0317 -0.0370 -0.0725 -0.0042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,line in enumerate(open(\"crawl-300d-2M.vec\",encoding=\"utf8\")):\n",
    "    if i ==1:\n",
    "        print(i,line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is .vec file shared by Analytics vidhya..but i see the file name as pretrained.vec in video. So we will use this .vec file and proceed for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999995 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "for i, line in enumerate(open(\"crawl-300d-2M.vec\",encoding=\"utf8\")):\n",
    "    value = line.split()\n",
    "    print(line)\n",
    "    break\n",
    "    embeddings_index[value[0]] = values[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First line contains the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", -0.0282 -0.0557 -0.0451 -0.0434 0.0712 -0.0855 -0.1085 -0.0561 -0.4523 -0.0202 0.0975 0.1047 0.1962 -0.0693 0.0213 -0.0235 0.1336 -0.0420 -0.0564 -0.0798 0.0424 -0.0409 -0.0536 -0.0252 0.0135 0.0064 0.1235 0.0461 0.0120 -0.0372 0.0650 0.0041 -0.1074 -0.0263 0.1133 -0.0029 0.0671 0.1065 0.0234 -0.0160 0.0070 0.4355 -0.0752 -0.4328 0.0457 0.0604 -0.0740 -0.0055 -0.0089 -0.2926 -0.0545 -0.1519 0.0990 -0.0193 -0.0050 0.0511 0.0404 0.1023 -0.0128 0.0488 -0.1567 -0.0759 -0.0190 0.1442 0.0047 -0.0186 0.0140 -0.0385 -0.0853 0.1572 0.1770 0.0084 -0.0250 -0.1145 -0.0663 -0.1244 -0.3977 -0.0124 -0.4586 -0.0220 0.5746 0.0218 -0.0754 0.0099 0.0397 -0.0154 0.0424 -0.0150 -0.0016 0.0305 0.0101 0.2266 0.1394 0.0189 0.0069 0.0394 0.0355 -0.0111 -0.0687 -0.0078 0.0224 0.0817 -0.1949 0.0001 0.4047 -0.0237 -0.0656 -0.0684 0.0233 0.0438 0.1203 -0.0276 0.0416 0.0114 -0.4529 0.1538 0.1323 -0.0186 -0.0914 -0.0312 0.1051 0.0212 0.0798 -0.0104 -0.0206 -0.0025 0.0043 -0.0378 0.2689 0.0747 -0.0418 -0.0048 -0.0387 0.0432 0.1704 0.0614 0.0905 -0.0436 -0.0141 -0.0315 0.0276 0.0151 -0.0103 -0.0266 -0.0512 -0.0408 -0.0651 0.0662 -0.0936 0.1371 0.0458 -0.1366 -0.0075 -0.0104 -0.0732 0.1205 0.1035 0.0106 -0.0317 -0.0316 0.6639 -0.0022 -0.1343 0.0144 -0.0338 0.0034 -0.0429 -0.0821 0.0037 0.1029 -0.0204 -0.0269 0.0052 -0.1034 0.1068 0.0121 0.0980 -0.0458 0.0199 -0.0132 0.1936 -0.0213 0.0209 -0.0025 0.0416 -0.0337 0.0516 -0.1014 0.0203 0.0198 -0.0305 -0.0313 0.0543 -0.0106 0.1441 -0.0178 -0.0627 0.0475 0.0352 -0.0254 -0.0949 0.0401 0.0317 0.0055 -0.0536 0.0191 -0.0511 -0.0409 -0.0030 0.1582 0.0108 0.5237 0.0436 0.0306 -0.0392 0.0177 0.0069 0.0605 0.1206 -0.0216 -0.0633 -0.2965 0.0521 -0.0150 -0.2207 -0.0642 -0.0906 -0.0121 0.0569 0.0944 -0.0652 -0.0108 -0.0477 0.0023 0.0077 -0.1547 0.0463 0.0698 -0.0376 -0.0291 0.0033 -0.0102 -0.0743 0.0085 0.0805 -0.0291 -0.0674 -0.0586 -0.0653 0.0283 -0.0255 0.0869 -0.0868 0.0090 0.3245 -0.0573 -0.0289 0.0470 -0.0117 0.0174 0.0132 -0.0226 -0.0664 0.0188 0.0263 0.0111 -0.0049 -0.0656 0.0295 0.0435 0.0290 0.1163 0.0448 -0.1139 -0.0553 -0.0528 0.1745 -0.0146 -0.1308 -0.0607 -0.0134 0.0781 0.0378 0.0228 -0.0728 -0.0059 0.0158 -0.0141 -0.0002 0.0193 -0.0148 -0.0463 0.0444 0.3034 0.1020 -0.0871 0.0317 -0.0370 -0.0725 -0.0042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "for i, line in enumerate(open(\"crawl-300d-2M.vec\",encoding=\"utf8\")):\n",
    "    if i ==0:\n",
    "        continue\n",
    "    value = line.split()\n",
    "    print(line)\n",
    "    break\n",
    "    embeddings_index[value[0]] = values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word in this case is symbol which is comma (,) and you can see the word vector for this comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {} # Embedding index which will contain what are the corresponding word vector values corresponding to a word\n",
    "\n",
    "\n",
    "\n",
    "# opening and iterating the file to see the values\n",
    "\n",
    "# In video, they mentioned pretrained vec contains word vectors for 2 million words\n",
    "\n",
    "for i, line in enumerate(open(\"crawl-300d-2M.vec\",encoding=\"utf8\")):\n",
    "    if i ==0:\n",
    "        continue\n",
    "    value = line.split()\n",
    "    embeddings_index[value[0]] = np.array(value[1:],dtype =\"float32\")\n",
    "    \n",
    "                                            # passing value of 0 into from the first element to the last element\n",
    "                                            # what we are doing here the line essentially contains first element as word and\n",
    "                                            # next element as word vectors\n",
    "                                            # converted into numpy instead of raw strings\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all of our words are loaded and they will be stored into word embeddings index dictionary (word as key and corresponding values as their word vector notations). This step will take time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step : Convert the text data into these word embedding representations\n",
    "\n",
    "from keras.preprocessing import text,sequence\n",
    "\n",
    "\n",
    "# Step 1: Convert data into tokens\n",
    "\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(data[\"text\"])\n",
    "word_index = token.word_index\n",
    "\n",
    "\n",
    "# Step 2: Convert text into sequence of tokens and padd them\n",
    "\n",
    "# This will generate sequence of words as the input and pad them to the equal length (70)\n",
    "\n",
    "\n",
    "trainx, valx, trainy, valy = train_test_split(data[\"text\"],target)\n",
    "\n",
    "trainx = sequence.pad_sequences(token.texts_to_sequences(trainx), maxlen = 70) # maxlength is given as 70\n",
    "\n",
    "valx = sequence.pad_sequences(token.texts_to_sequences(valx),maxlen = 70)\n",
    "\n",
    "\n",
    "# Step 3: Create an embedding matrix which corresponds to different embedding vectors for different key words present in every sentence\n",
    "\n",
    "# 300 is length of word vector provided for the pre trained models, look at the dimensions\n",
    "\n",
    "# This embedding matrix is similar to document word matrix (rows x columns = word X vector notations)\n",
    "\n",
    "#embedding_matrix = np.zeros((len(word_index)+1),300)) # this code was throwing error\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "# To fill this matrix, we need to iterate word by word\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word) # fetches embedding vector for each word\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'to': 2,\n",
       " 'you': 3,\n",
       " 'a': 4,\n",
       " 'the': 5,\n",
       " 'u': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'is': 9,\n",
       " 'me': 10,\n",
       " 'my': 11,\n",
       " 'for': 12,\n",
       " 'your': 13,\n",
       " 'it': 14,\n",
       " 'of': 15,\n",
       " 'call': 16,\n",
       " 'have': 17,\n",
       " 'on': 18,\n",
       " '2': 19,\n",
       " 'that': 20,\n",
       " 'now': 21,\n",
       " 'are': 22,\n",
       " 'so': 23,\n",
       " 'but': 24,\n",
       " 'not': 25,\n",
       " 'or': 26,\n",
       " 'do': 27,\n",
       " 'can': 28,\n",
       " 'at': 29,\n",
       " \"i'm\": 30,\n",
       " 'get': 31,\n",
       " 'be': 32,\n",
       " 'will': 33,\n",
       " 'if': 34,\n",
       " 'ur': 35,\n",
       " 'with': 36,\n",
       " 'just': 37,\n",
       " 'no': 38,\n",
       " 'we': 39,\n",
       " 'this': 40,\n",
       " 'gt': 41,\n",
       " '4': 42,\n",
       " 'lt': 43,\n",
       " 'up': 44,\n",
       " 'when': 45,\n",
       " 'ok': 46,\n",
       " 'free': 47,\n",
       " 'from': 48,\n",
       " 'how': 49,\n",
       " 'go': 50,\n",
       " 'all': 51,\n",
       " 'out': 52,\n",
       " 'what': 53,\n",
       " 'know': 54,\n",
       " 'like': 55,\n",
       " 'good': 56,\n",
       " 'then': 57,\n",
       " 'got': 58,\n",
       " 'was': 59,\n",
       " 'come': 60,\n",
       " 'its': 61,\n",
       " 'am': 62,\n",
       " 'time': 63,\n",
       " 'only': 64,\n",
       " 'day': 65,\n",
       " 'love': 66,\n",
       " 'there': 67,\n",
       " 'send': 68,\n",
       " 'he': 69,\n",
       " 'want': 70,\n",
       " 'text': 71,\n",
       " 'as': 72,\n",
       " 'txt': 73,\n",
       " 'one': 74,\n",
       " 'going': 75,\n",
       " 'by': 76,\n",
       " 'home': 77,\n",
       " \"i'll\": 78,\n",
       " 'need': 79,\n",
       " 'about': 80,\n",
       " 'r': 81,\n",
       " 'lor': 82,\n",
       " 'sorry': 83,\n",
       " 'stop': 84,\n",
       " 'still': 85,\n",
       " 'see': 86,\n",
       " 'back': 87,\n",
       " 'today': 88,\n",
       " 'n': 89,\n",
       " 'da': 90,\n",
       " 'our': 91,\n",
       " 'reply': 92,\n",
       " 'k': 93,\n",
       " 'dont': 94,\n",
       " 'she': 95,\n",
       " 'mobile': 96,\n",
       " 'take': 97,\n",
       " \"don't\": 98,\n",
       " 'tell': 99,\n",
       " 'hi': 100,\n",
       " 'new': 101,\n",
       " 'later': 102,\n",
       " 'her': 103,\n",
       " 'pls': 104,\n",
       " 'any': 105,\n",
       " 'please': 106,\n",
       " 'think': 107,\n",
       " 'been': 108,\n",
       " 'they': 109,\n",
       " 'phone': 110,\n",
       " 'here': 111,\n",
       " 'week': 112,\n",
       " 'dear': 113,\n",
       " 'did': 114,\n",
       " 'some': 115,\n",
       " 'ã\\x8c': 116,\n",
       " '1': 117,\n",
       " 'well': 118,\n",
       " 'has': 119,\n",
       " 'much': 120,\n",
       " 'great': 121,\n",
       " 'night': 122,\n",
       " 'oh': 123,\n",
       " 'claim': 124,\n",
       " 'an': 125,\n",
       " 'hope': 126,\n",
       " 'hey': 127,\n",
       " 'msg': 128,\n",
       " 'who': 129,\n",
       " 'him': 130,\n",
       " 'where': 131,\n",
       " 'd': 132,\n",
       " 'more': 133,\n",
       " 'too': 134,\n",
       " 'happy': 135,\n",
       " 'had': 136,\n",
       " 'yes': 137,\n",
       " 'make': 138,\n",
       " 'way': 139,\n",
       " 'c': 140,\n",
       " 'www': 141,\n",
       " 'work': 142,\n",
       " 'give': 143,\n",
       " 'wat': 144,\n",
       " \"it's\": 145,\n",
       " 'number': 146,\n",
       " 'e': 147,\n",
       " 'message': 148,\n",
       " 'should': 149,\n",
       " 'prize': 150,\n",
       " 'tomorrow': 151,\n",
       " 'say': 152,\n",
       " 'right': 153,\n",
       " 'already': 154,\n",
       " 'after': 155,\n",
       " 'ask': 156,\n",
       " 'cash': 157,\n",
       " 'doing': 158,\n",
       " 'said': 159,\n",
       " '3': 160,\n",
       " 'yeah': 161,\n",
       " 'really': 162,\n",
       " 'amp': 163,\n",
       " 'why': 164,\n",
       " 'im': 165,\n",
       " 'meet': 166,\n",
       " 'them': 167,\n",
       " 'life': 168,\n",
       " 'find': 169,\n",
       " 'very': 170,\n",
       " 'morning': 171,\n",
       " 'babe': 172,\n",
       " 'last': 173,\n",
       " 'miss': 174,\n",
       " 'thanks': 175,\n",
       " 'would': 176,\n",
       " 'cos': 177,\n",
       " 'win': 178,\n",
       " 't': 179,\n",
       " 'lol': 180,\n",
       " 'also': 181,\n",
       " 'won': 182,\n",
       " 'let': 183,\n",
       " 'b': 184,\n",
       " 'anything': 185,\n",
       " 'every': 186,\n",
       " '150p': 187,\n",
       " 'com': 188,\n",
       " 'sure': 189,\n",
       " 'pick': 190,\n",
       " 'care': 191,\n",
       " 'urgent': 192,\n",
       " 'nokia': 193,\n",
       " 'sent': 194,\n",
       " 'keep': 195,\n",
       " 'over': 196,\n",
       " 'uk': 197,\n",
       " 'something': 198,\n",
       " 'contact': 199,\n",
       " 'us': 200,\n",
       " 'again': 201,\n",
       " 'buy': 202,\n",
       " 'min': 203,\n",
       " 'wait': 204,\n",
       " 'cant': 205,\n",
       " 'before': 206,\n",
       " \"i've\": 207,\n",
       " 'first': 208,\n",
       " 's': 209,\n",
       " '5': 210,\n",
       " 'even': 211,\n",
       " 'next': 212,\n",
       " 'feel': 213,\n",
       " 'were': 214,\n",
       " 'nice': 215,\n",
       " 'went': 216,\n",
       " 'thing': 217,\n",
       " 'around': 218,\n",
       " 'soon': 219,\n",
       " 'his': 220,\n",
       " 'which': 221,\n",
       " 'someone': 222,\n",
       " \"can't\": 223,\n",
       " 'could': 224,\n",
       " 'place': 225,\n",
       " 'money': 226,\n",
       " 'service': 227,\n",
       " 'off': 228,\n",
       " 'tone': 229,\n",
       " '50': 230,\n",
       " 'tonight': 231,\n",
       " 'late': 232,\n",
       " 'many': 233,\n",
       " 'per': 234,\n",
       " 'customer': 235,\n",
       " 'gonna': 236,\n",
       " 'chat': 237,\n",
       " 'ya': 238,\n",
       " 'sleep': 239,\n",
       " 'always': 240,\n",
       " 'leave': 241,\n",
       " 'co': 242,\n",
       " 'down': 243,\n",
       " 'sms': 244,\n",
       " 'dun': 245,\n",
       " 'friends': 246,\n",
       " 'v': 247,\n",
       " \"that's\": 248,\n",
       " 'gud': 249,\n",
       " 'other': 250,\n",
       " 'wan': 251,\n",
       " 'help': 252,\n",
       " 'x': 253,\n",
       " 'things': 254,\n",
       " 'told': 255,\n",
       " 'wish': 256,\n",
       " 'hello': 257,\n",
       " 'waiting': 258,\n",
       " '16': 259,\n",
       " 'ã\\x8cã\\x8f': 260,\n",
       " 'fine': 261,\n",
       " 'special': 262,\n",
       " '18': 263,\n",
       " \"you're\": 264,\n",
       " 'haha': 265,\n",
       " 'coming': 266,\n",
       " 'may': 267,\n",
       " 'name': 268,\n",
       " 'getting': 269,\n",
       " 'done': 270,\n",
       " 'year': 271,\n",
       " 'same': 272,\n",
       " 'guaranteed': 273,\n",
       " 'yet': 274,\n",
       " 'people': 275,\n",
       " 'thk': 276,\n",
       " 'use': 277,\n",
       " 'try': 278,\n",
       " 'friend': 279,\n",
       " 'mins': 280,\n",
       " 'heart': 281,\n",
       " 'thought': 282,\n",
       " '6': 283,\n",
       " 'holiday': 284,\n",
       " 'lunch': 285,\n",
       " 'live': 286,\n",
       " 'man': 287,\n",
       " 'best': 288,\n",
       " 'talk': 289,\n",
       " 'stuff': 290,\n",
       " 'class': 291,\n",
       " 'y': 292,\n",
       " 'smile': 293,\n",
       " \"didn't\": 294,\n",
       " 'draw': 295,\n",
       " 'few': 296,\n",
       " 'cs': 297,\n",
       " 'days': 298,\n",
       " '7': 299,\n",
       " 'being': 300,\n",
       " 'yup': 301,\n",
       " 'trying': 302,\n",
       " 'bit': 303,\n",
       " 'never': 304,\n",
       " 'meeting': 305,\n",
       " 'thats': 306,\n",
       " 'job': 307,\n",
       " 'better': 308,\n",
       " 'house': 309,\n",
       " 'line': 310,\n",
       " 'finish': 311,\n",
       " 'cool': 312,\n",
       " 'long': 313,\n",
       " 'ill': 314,\n",
       " 'ready': 315,\n",
       " 'person': 316,\n",
       " 'having': 317,\n",
       " 'car': 318,\n",
       " 'mind': 319,\n",
       " 'end': 320,\n",
       " 'enjoy': 321,\n",
       " 'ã¥â£1': 322,\n",
       " 'latest': 323,\n",
       " 'half': 324,\n",
       " 'play': 325,\n",
       " 'check': 326,\n",
       " 'real': 327,\n",
       " 'yo': 328,\n",
       " 'wk': 329,\n",
       " 'account': 330,\n",
       " 'because': 331,\n",
       " 'dat': 332,\n",
       " 'than': 333,\n",
       " 'chance': 334,\n",
       " 'god': 335,\n",
       " 'lar': 336,\n",
       " 'receive': 337,\n",
       " 'word': 338,\n",
       " 'camera': 339,\n",
       " 'eat': 340,\n",
       " 'awarded': 341,\n",
       " 'wanna': 342,\n",
       " 'nothing': 343,\n",
       " 'guess': 344,\n",
       " 'lot': 345,\n",
       " 'sir': 346,\n",
       " 'problem': 347,\n",
       " '1st': 348,\n",
       " 'world': 349,\n",
       " 'another': 350,\n",
       " 'liao': 351,\n",
       " 'big': 352,\n",
       " 'dinner': 353,\n",
       " 'month': 354,\n",
       " 'ah': 355,\n",
       " 'birthday': 356,\n",
       " 'shows': 357,\n",
       " 'guys': 358,\n",
       " 'start': 359,\n",
       " 'into': 360,\n",
       " 'shit': 361,\n",
       " 'sweet': 362,\n",
       " 'ã¥â£1000': 363,\n",
       " 'girl': 364,\n",
       " 'luv': 365,\n",
       " 'jus': 366,\n",
       " 'might': 367,\n",
       " 'box': 368,\n",
       " 'ever': 369,\n",
       " 'quite': 370,\n",
       " 'cost': 371,\n",
       " 'watching': 372,\n",
       " 'room': 373,\n",
       " '150ppm': 374,\n",
       " 'landline': 375,\n",
       " 'bt': 376,\n",
       " 'offer': 377,\n",
       " 'video': 378,\n",
       " 'early': 379,\n",
       " 'xxx': 380,\n",
       " 'speak': 381,\n",
       " 'once': 382,\n",
       " 'aight': 383,\n",
       " 'tv': 384,\n",
       " 'called': 385,\n",
       " 'watch': 386,\n",
       " 'probably': 387,\n",
       " 'rate': 388,\n",
       " 'apply': 389,\n",
       " 'wont': 390,\n",
       " 'remember': 391,\n",
       " 'does': 392,\n",
       " 'maybe': 393,\n",
       " 'hear': 394,\n",
       " 'pa': 395,\n",
       " 'bed': 396,\n",
       " 'forgot': 397,\n",
       " 'll': 398,\n",
       " 'boy': 399,\n",
       " 'po': 400,\n",
       " 'thanx': 401,\n",
       " 'plan': 402,\n",
       " 'shall': 403,\n",
       " 'two': 404,\n",
       " 'minutes': 405,\n",
       " 'sat': 406,\n",
       " 'actually': 407,\n",
       " 'den': 408,\n",
       " 'bad': 409,\n",
       " 'princess': 410,\n",
       " 'fun': 411,\n",
       " '9': 412,\n",
       " 'code': 413,\n",
       " 'pay': 414,\n",
       " 'left': 415,\n",
       " 'ringtone': 416,\n",
       " 'look': 417,\n",
       " 'weekend': 418,\n",
       " 'part': 419,\n",
       " 'between': 420,\n",
       " 'easy': 421,\n",
       " 'reach': 422,\n",
       " 'shopping': 423,\n",
       " 'baby': 424,\n",
       " 'dunno': 425,\n",
       " 'orange': 426,\n",
       " 'office': 427,\n",
       " 'kiss': 428,\n",
       " '2nd': 429,\n",
       " \"he's\": 430,\n",
       " 'dis': 431,\n",
       " '10': 432,\n",
       " 'little': 433,\n",
       " 'leh': 434,\n",
       " 'face': 435,\n",
       " 'didnt': 436,\n",
       " 'hour': 437,\n",
       " 'network': 438,\n",
       " 'selected': 439,\n",
       " 'enough': 440,\n",
       " '000': 441,\n",
       " 'thank': 442,\n",
       " 'bus': 443,\n",
       " \"how's\": 444,\n",
       " 'looking': 445,\n",
       " 'anyway': 446,\n",
       " 'award': 447,\n",
       " 'those': 448,\n",
       " 'm': 449,\n",
       " 'working': 450,\n",
       " 'everything': 451,\n",
       " 'made': 452,\n",
       " 'put': 453,\n",
       " 'wife': 454,\n",
       " 'most': 455,\n",
       " 'afternoon': 456,\n",
       " 'without': 457,\n",
       " 'missing': 458,\n",
       " 'tmr': 459,\n",
       " 'evening': 460,\n",
       " 'collect': 461,\n",
       " 'asked': 462,\n",
       " 'texts': 463,\n",
       " '8': 464,\n",
       " 'while': 465,\n",
       " 'fuck': 466,\n",
       " 'dad': 467,\n",
       " 'town': 468,\n",
       " 'until': 469,\n",
       " 'wif': 470,\n",
       " 'though': 471,\n",
       " \"there's\": 472,\n",
       " 'calls': 473,\n",
       " 'since': 474,\n",
       " 'came': 475,\n",
       " 'okay': 476,\n",
       " 'says': 477,\n",
       " 'must': 478,\n",
       " 'school': 479,\n",
       " 'join': 480,\n",
       " 'mail': 481,\n",
       " 'sexy': 482,\n",
       " 'xmas': 483,\n",
       " 'true': 484,\n",
       " 'details': 485,\n",
       " 'entry': 486,\n",
       " 'goes': 487,\n",
       " 'update': 488,\n",
       " 'wanted': 489,\n",
       " 'pain': 490,\n",
       " 'means': 491,\n",
       " 'abt': 492,\n",
       " 'able': 493,\n",
       " 'hav': 494,\n",
       " 'important': 495,\n",
       " 'g': 496,\n",
       " 'wake': 497,\n",
       " 'tones': 498,\n",
       " 'wot': 499,\n",
       " 'bring': 500,\n",
       " 'collection': 501,\n",
       " 'times': 502,\n",
       " 'messages': 503,\n",
       " 'missed': 504,\n",
       " 'mob': 505,\n",
       " 'show': 506,\n",
       " 'price': 507,\n",
       " 'juz': 508,\n",
       " 'years': 509,\n",
       " 'decimal': 510,\n",
       " 'plz': 511,\n",
       " 'de': 512,\n",
       " 'away': 513,\n",
       " 'gift': 514,\n",
       " 'plus': 515,\n",
       " 'valid': 516,\n",
       " 'ã¥â£100': 517,\n",
       " 'alright': 518,\n",
       " 'till': 519,\n",
       " 're': 520,\n",
       " 'saw': 521,\n",
       " 'yesterday': 522,\n",
       " 'hair': 523,\n",
       " 'wen': 524,\n",
       " 'havent': 525,\n",
       " 'else': 526,\n",
       " 'worry': 527,\n",
       " '500': 528,\n",
       " '10p': 529,\n",
       " 'music': 530,\n",
       " 'weekly': 531,\n",
       " 'attempt': 532,\n",
       " 'guy': 533,\n",
       " 'colour': 534,\n",
       " 'net': 535,\n",
       " 'words': 536,\n",
       " 'yours': 537,\n",
       " 'double': 538,\n",
       " 'run': 539,\n",
       " 'making': 540,\n",
       " 'food': 541,\n",
       " 'haf': 542,\n",
       " 'til': 543,\n",
       " 'id': 544,\n",
       " 'oso': 545,\n",
       " 'shop': 546,\n",
       " 'book': 547,\n",
       " 'dude': 548,\n",
       " 'stay': 549,\n",
       " 'bored': 550,\n",
       " 'online': 551,\n",
       " 'makes': 552,\n",
       " 'lei': 553,\n",
       " 'question': 554,\n",
       " 'national': 555,\n",
       " 'ard': 556,\n",
       " \"we're\": 557,\n",
       " \"won't\": 558,\n",
       " 'tried': 559,\n",
       " 'delivery': 560,\n",
       " 'yourself': 561,\n",
       " \"haven't\": 562,\n",
       " 'driving': 563,\n",
       " 'test': 564,\n",
       " 'address': 565,\n",
       " 'answer': 566,\n",
       " 'top': 567,\n",
       " 'coz': 568,\n",
       " \"what's\": 569,\n",
       " 'nite': 570,\n",
       " 'hot': 571,\n",
       " 'hurt': 572,\n",
       " 'friendship': 573,\n",
       " 'change': 574,\n",
       " 'feeling': 575,\n",
       " 'either': 576,\n",
       " 'these': 577,\n",
       " 'sch': 578,\n",
       " 'family': 579,\n",
       " 'goin': 580,\n",
       " 'hours': 581,\n",
       " 'date': 582,\n",
       " 'http': 583,\n",
       " 'bonus': 584,\n",
       " 'trip': 585,\n",
       " 'comes': 586,\n",
       " 'ã¥â£5000': 587,\n",
       " 'movie': 588,\n",
       " 'busy': 589,\n",
       " \"''\": 590,\n",
       " 'todays': 591,\n",
       " 'order': 592,\n",
       " 'believe': 593,\n",
       " 'both': 594,\n",
       " 'vouchers': 595,\n",
       " 'wid': 596,\n",
       " 'full': 597,\n",
       " 'calling': 598,\n",
       " 'tot': 599,\n",
       " 'beautiful': 600,\n",
       " 'sae': 601,\n",
       " 'lose': 602,\n",
       " 'game': 603,\n",
       " 'together': 604,\n",
       " 'wants': 605,\n",
       " '8007': 606,\n",
       " 'sad': 607,\n",
       " 'set': 608,\n",
       " 'smiling': 609,\n",
       " 'mean': 610,\n",
       " 'old': 611,\n",
       " 'points': 612,\n",
       " 'ã¥â£2000': 613,\n",
       " 'leaving': 614,\n",
       " 'story': 615,\n",
       " 'sleeping': 616,\n",
       " 'noe': 617,\n",
       " 'happen': 618,\n",
       " 'ring': 619,\n",
       " 'club': 620,\n",
       " 'charge': 621,\n",
       " 'games': 622,\n",
       " \"we'll\": 623,\n",
       " 'chikku': 624,\n",
       " 'huh': 625,\n",
       " 'eve': 626,\n",
       " 'ã¥â£500': 627,\n",
       " 'saying': 628,\n",
       " 'drive': 629,\n",
       " 'await': 630,\n",
       " 'dreams': 631,\n",
       " 'brother': 632,\n",
       " 'pounds': 633,\n",
       " 'news': 634,\n",
       " 'aft': 635,\n",
       " 'tomo': 636,\n",
       " 'congrats': 637,\n",
       " 'took': 638,\n",
       " 'finished': 639,\n",
       " 'started': 640,\n",
       " 'private': 641,\n",
       " 'gr8': 642,\n",
       " 'awesome': 643,\n",
       " 'minute': 644,\n",
       " 'walk': 645,\n",
       " '86688': 646,\n",
       " 'okie': 647,\n",
       " 'post': 648,\n",
       " 'row': 649,\n",
       " 'poly': 650,\n",
       " 'pm': 651,\n",
       " 'thinking': 652,\n",
       " 'pics': 653,\n",
       " 'email': 654,\n",
       " 'rite': 655,\n",
       " 'pic': 656,\n",
       " 'available': 657,\n",
       " 'final': 658,\n",
       " \"c's\": 659,\n",
       " 'tho': 660,\n",
       " 'forget': 661,\n",
       " 'second': 662,\n",
       " 'close': 663,\n",
       " 'cause': 664,\n",
       " 'services': 665,\n",
       " 'taking': 666,\n",
       " 'everyone': 667,\n",
       " 'wil': 668,\n",
       " 'angry': 669,\n",
       " '750': 670,\n",
       " 'unsubscribe': 671,\n",
       " 'lets': 672,\n",
       " 'drink': 673,\n",
       " 'head': 674,\n",
       " 'land': 675,\n",
       " 'gd': 676,\n",
       " 'neva': 677,\n",
       " 'pub': 678,\n",
       " \"she's\": 679,\n",
       " 'drop': 680,\n",
       " 'auction': 681,\n",
       " '11': 682,\n",
       " 'lesson': 683,\n",
       " 'lucky': 684,\n",
       " 'xx': 685,\n",
       " 'search': 686,\n",
       " '12hrs': 687,\n",
       " 'statement': 688,\n",
       " 'expires': 689,\n",
       " 'msgs': 690,\n",
       " 'open': 691,\n",
       " 'whats': 692,\n",
       " 'lots': 693,\n",
       " 'each': 694,\n",
       " 'smoke': 695,\n",
       " 'worth': 696,\n",
       " 'sis': 697,\n",
       " 'touch': 698,\n",
       " 'found': 699,\n",
       " 'break': 700,\n",
       " 'sounds': 701,\n",
       " 'company': 702,\n",
       " 'choose': 703,\n",
       " 'card': 704,\n",
       " 'w': 705,\n",
       " 'sister': 706,\n",
       " 'dating': 707,\n",
       " 'opt': 708,\n",
       " 'simple': 709,\n",
       " 'mine': 710,\n",
       " 'whatever': 711,\n",
       " 'voucher': 712,\n",
       " 'knw': 713,\n",
       " 'anyone': 714,\n",
       " 'don': 715,\n",
       " 'loving': 716,\n",
       " 'alone': 717,\n",
       " 'treat': 718,\n",
       " 'winner': 719,\n",
       " '100': 720,\n",
       " 'info': 721,\n",
       " 'pobox': 722,\n",
       " 'ha': 723,\n",
       " 'smth': 724,\n",
       " 'saturday': 725,\n",
       " 'decided': 726,\n",
       " '08000930705': 727,\n",
       " 'girls': 728,\n",
       " 'prob': 729,\n",
       " 'gone': 730,\n",
       " 'happened': 731,\n",
       " 'identifier': 732,\n",
       " 'nt': 733,\n",
       " 'type': 734,\n",
       " 'ltd': 735,\n",
       " 'hard': 736,\n",
       " 'frnd': 737,\n",
       " 'needs': 738,\n",
       " 'carlos': 739,\n",
       " 'boytoy': 740,\n",
       " 'college': 741,\n",
       " 'takes': 742,\n",
       " 'anytime': 743,\n",
       " 'far': 744,\n",
       " 'mobileupd8': 745,\n",
       " 'kind': 746,\n",
       " 'visit': 747,\n",
       " 'fast': 748,\n",
       " 'mum': 749,\n",
       " 'sun': 750,\n",
       " 'crazy': 751,\n",
       " 'wonderful': 752,\n",
       " \"doesn't\": 753,\n",
       " 'camcorder': 754,\n",
       " 'used': 755,\n",
       " 'hit': 756,\n",
       " 'operator': 757,\n",
       " 'friday': 758,\n",
       " 'quiz': 759,\n",
       " 'player': 760,\n",
       " 'parents': 761,\n",
       " 'hand': 762,\n",
       " 'content': 763,\n",
       " 'wit': 764,\n",
       " \"you've\": 765,\n",
       " 'finally': 766,\n",
       " 'darlin': 767,\n",
       " 'rs': 768,\n",
       " 'goodmorning': 769,\n",
       " 'oredi': 770,\n",
       " 'secret': 771,\n",
       " 'tel': 772,\n",
       " 'congratulations': 773,\n",
       " 'read': 774,\n",
       " 'light': 775,\n",
       " 'suite342': 776,\n",
       " '2lands': 777,\n",
       " '08000839402': 778,\n",
       " 'bout': 779,\n",
       " 'fucking': 780,\n",
       " 'nope': 781,\n",
       " 'outside': 782,\n",
       " 'fri': 783,\n",
       " 'ã¥â£3': 784,\n",
       " 'pretty': 785,\n",
       " 'sea': 786,\n",
       " 'o': 787,\n",
       " 'weeks': 788,\n",
       " 'â\\x80°ã\\x9b': 789,\n",
       " 'lovely': 790,\n",
       " 'mates': 791,\n",
       " 'wrong': 792,\n",
       " 'chennai': 793,\n",
       " 'hows': 794,\n",
       " '30': 795,\n",
       " 'wkly': 796,\n",
       " 'freemsg': 797,\n",
       " \"'\": 798,\n",
       " 'sunday': 799,\n",
       " 'credit': 800,\n",
       " 'hungry': 801,\n",
       " 'seeing': 802,\n",
       " 'telling': 803,\n",
       " 'whole': 804,\n",
       " 'frnds': 805,\n",
       " 'hmm': 806,\n",
       " 'mu': 807,\n",
       " \"you'll\": 808,\n",
       " 'yr': 809,\n",
       " 'their': 810,\n",
       " 'ni8': 811,\n",
       " 'f': 812,\n",
       " 'fancy': 813,\n",
       " 'bank': 814,\n",
       " 'log': 815,\n",
       " 'course': 816,\n",
       " 'tc': 817,\n",
       " 'thinks': 818,\n",
       " 'case': 819,\n",
       " 'meant': 820,\n",
       " 'hold': 821,\n",
       " 'unlimited': 822,\n",
       " 'blue': 823,\n",
       " 'fone': 824,\n",
       " 'project': 825,\n",
       " 'reason': 826,\n",
       " 'ã¥â£250': 827,\n",
       " 'ten': 828,\n",
       " 'welcome': 829,\n",
       " 'cum': 830,\n",
       " 'frm': 831,\n",
       " 'savamob': 832,\n",
       " 'offers': 833,\n",
       " 'listen': 834,\n",
       " 'snow': 835,\n",
       " 'b4': 836,\n",
       " 'mate': 837,\n",
       " 'least': 838,\n",
       " 'earlier': 839,\n",
       " 'party': 840,\n",
       " 'point': 841,\n",
       " 'press': 842,\n",
       " 'valued': 843,\n",
       " 'almost': 844,\n",
       " 'etc': 845,\n",
       " 'cut': 846,\n",
       " 'hee': 847,\n",
       " 'download': 848,\n",
       " '0800': 849,\n",
       " 'mah': 850,\n",
       " 'felt': 851,\n",
       " 'caller': 852,\n",
       " '03': 853,\n",
       " 'numbers': 854,\n",
       " 'age': 855,\n",
       " 'tired': 856,\n",
       " 'hmmm': 857,\n",
       " 'mr': 858,\n",
       " 'mrng': 859,\n",
       " 'balance': 860,\n",
       " 'march': 861,\n",
       " 'side': 862,\n",
       " 'fr': 863,\n",
       " '87066': 864,\n",
       " 'dnt': 865,\n",
       " 'stupid': 866,\n",
       " 'bslvyl': 867,\n",
       " 'lost': 868,\n",
       " 'christmas': 869,\n",
       " 'reading': 870,\n",
       " 'txts': 871,\n",
       " 'ago': 872,\n",
       " 'currently': 873,\n",
       " 'motorola': 874,\n",
       " 'talking': 875,\n",
       " 'couple': 876,\n",
       " 'phones': 877,\n",
       " 'ass': 878,\n",
       " 'india': 879,\n",
       " 'park': 880,\n",
       " 'ã¥â£2': 881,\n",
       " 'within': 882,\n",
       " '2003': 883,\n",
       " '800': 884,\n",
       " 'un': 885,\n",
       " 'yar': 886,\n",
       " 'happiness': 887,\n",
       " 'area': 888,\n",
       " 'ã¥â£350': 889,\n",
       " 'sex': 890,\n",
       " 'mayb': 891,\n",
       " 'understand': 892,\n",
       " 'support': 893,\n",
       " 'na': 894,\n",
       " 'luck': 895,\n",
       " 'enter': 896,\n",
       " 'gas': 897,\n",
       " 'father': 898,\n",
       " 'comp': 899,\n",
       " \"i'd\": 900,\n",
       " 'mobiles': 901,\n",
       " '20': 902,\n",
       " 'eh': 903,\n",
       " 'charged': 904,\n",
       " 'confirm': 905,\n",
       " 'wow': 906,\n",
       " 'ac': 907,\n",
       " 'red': 908,\n",
       " 'correct': 909,\n",
       " 'pass': 910,\n",
       " 'song': 911,\n",
       " 'complimentary': 912,\n",
       " 'gotta': 913,\n",
       " 'computer': 914,\n",
       " 'mom': 915,\n",
       " 'askd': 916,\n",
       " 'invited': 917,\n",
       " 'uncle': 918,\n",
       " 'sending': 919,\n",
       " 'direct': 920,\n",
       " 'semester': 921,\n",
       " 'reveal': 922,\n",
       " 'laptop': 923,\n",
       " 'questions': 924,\n",
       " 'swing': 925,\n",
       " 'ends': 926,\n",
       " 'die': 927,\n",
       " 'via': 928,\n",
       " 'met': 929,\n",
       " 'st': 930,\n",
       " 'call2optout': 931,\n",
       " 'seen': 932,\n",
       " 'rental': 933,\n",
       " 'th': 934,\n",
       " 'supposed': 935,\n",
       " 'ipod': 936,\n",
       " 'redeemed': 937,\n",
       " '04': 938,\n",
       " 'through': 939,\n",
       " 'gym': 940,\n",
       " 'darren': 941,\n",
       " 'ans': 942,\n",
       " 'picking': 943,\n",
       " 'ugh': 944,\n",
       " 'extra': 945,\n",
       " 'knew': 946,\n",
       " 'heard': 947,\n",
       " 'information': 948,\n",
       " 'surprise': 949,\n",
       " 'grins': 950,\n",
       " 'gal': 951,\n",
       " 'difficult': 952,\n",
       " 'john': 953,\n",
       " \"wasn't\": 954,\n",
       " 'std': 955,\n",
       " 'usf': 956,\n",
       " 'reward': 957,\n",
       " '12': 958,\n",
       " 'wap': 959,\n",
       " 'eg': 960,\n",
       " 'comin': 961,\n",
       " 'abiola': 962,\n",
       " 'crave': 963,\n",
       " 'gets': 964,\n",
       " 'move': 965,\n",
       " 'checking': 966,\n",
       " 'rply': 967,\n",
       " 'loads': 968,\n",
       " 'shower': 969,\n",
       " \"isn't\": 970,\n",
       " 'entered': 971,\n",
       " 'match': 972,\n",
       " 'dogging': 973,\n",
       " 'txting': 974,\n",
       " 'lovable': 975,\n",
       " 'wine': 976,\n",
       " 'dream': 977,\n",
       " 'safe': 978,\n",
       " 'muz': 979,\n",
       " 'bath': 980,\n",
       " 'orchard': 981,\n",
       " 'kate': 982,\n",
       " 'exam': 983,\n",
       " 'bcoz': 984,\n",
       " 'own': 985,\n",
       " 'wana': 986,\n",
       " 'somebody': 987,\n",
       " 'rest': 988,\n",
       " 'plans': 989,\n",
       " 'small': 990,\n",
       " 'jay': 991,\n",
       " 'ex': 992,\n",
       " 'hg': 993,\n",
       " 'w1j6hl': 994,\n",
       " 'discount': 995,\n",
       " 'slow': 996,\n",
       " 'rock': 997,\n",
       " 'asking': 998,\n",
       " 'remove': 999,\n",
       " 'monday': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index # This gives info like which word corresponds to which index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.48210001,  0.0885    , -0.0782    , ..., -0.0317    ,\n",
       "        -0.1591    , -0.1301    ],\n",
       "       [-0.0175    , -0.2189    ,  0.0353    , ..., -0.28459999,\n",
       "         0.0509    ,  0.0229    ],\n",
       "       ...,\n",
       "       [ 0.1224    ,  0.0217    , -0.34729999, ..., -0.54530001,\n",
       "        -0.0505    , -0.1023    ],\n",
       "       [ 0.1039    , -0.37940001, -0.0344    , ...,  0.0994    ,\n",
       "        -0.25979999, -0.17110001],\n",
       "       [ 0.32229999, -0.42629999,  0.4325    , ...,  0.14229999,\n",
       "         0.0177    ,  0.0414    ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix # All of the words have been essentially filled into embedding matrix. Inside this word and corresponding values\n",
    "                 # will be present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Step: Training the model\n",
    "\n",
    "def train_model(classifier, feature_vector_train,label,feature_vector_val,valid_y): \n",
    "    classifier.fit(feature_vector_train,label)\n",
    "    predictions = classifier.predict(feature_vector_val)\n",
    "    predictions = predictions.argmax(axis = -1) # converts the probabilities into classes\n",
    "    return accuracy_score(predictions,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing CNN as Classifier\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "def create_cnn():\n",
    "    \n",
    "    input_layer = layers.Input(70) # First Layer. In previous steps, we defined input size as 70\n",
    "                                   # Every sequence is pallet sequence which contains 70 element at max\n",
    "    \n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights = [embedding_matrix],trainable = False)(input_layer)\n",
    "                        \n",
    "                                # 300 is the length of the word vectors\n",
    "                                # Since we are using pre-trained word model we will pass those weights to this layer\n",
    "                                # trainable = False, we don't want to train our own word vector because we are using pre-trained model\n",
    "    \n",
    "    conv_layer = layers.Convolution1D(100, 3, activation =\"relu\")(embedding_layer)\n",
    "    \n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "    \n",
    "    # This dense part will be like a Neural Network\n",
    "    \n",
    "    output_layer = layers.Dense(50, activation =\"relu\") (pooling_layer) # feature extraction happens here. Conolutional features\n",
    "    output_layer = layers.Dropout(0.25) (output_layer)          # Dropout is 25%. This is nothing but Regularization in NN   \n",
    "    output_layer = layers.Dense(1, activation =\"sigmoid\") (output_layer) # clasification layer. Sigmoid ensures the range 0 to 1\n",
    "    \n",
    "    \n",
    "    model = models.Model(inputs = input_layer,outputs = output_layer)\n",
    "    # model.compile(optmizer = optimizers.Adam(), loss = \"binary_crossentropy\")\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "                                    \n",
    "                                                        # We can also use Adamdelta or RMSProp instead of Adam\n",
    "                                                        # Since it is binary classification hence it is binary_crossentropy\n",
    "    return models \n",
    "                                                        # Layers are connected (previous layer name is mentioned in the line)\n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FF601B6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FF601B6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.1897: 0s - loss: 0.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FF60C2CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FF60C2CB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8722182340272793"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = create_cnn()\n",
    "\n",
    "train_model(classifier, trainx, trainy, valx, valy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I have mistakenly written like below...it was throwing error...hence it is important to understand to pass the paramters\n",
    "\n",
    " output_layer = layers.Dense(50, activation =\"sigmoid\") (output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have more data...more numbr of epochs can be done. If we the accuracy of CNN model is very less when compare to simple models. Deep Learning models works well with Large amount of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
