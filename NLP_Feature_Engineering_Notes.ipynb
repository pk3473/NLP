{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Algorithms cannot accept text as input hence this need to converted into appropriate featues.\n",
    "\n",
    "E.g. \"This course is teaching us NLP. NLP is field of Datascience. NLP means Natural Language Processing\"\n",
    "\n",
    "Extracted Features:\n",
    "    - Sentences = 3\n",
    "    - Words = 18\n",
    "    - Characters = 84\n",
    "    - Stopwords = 6\n",
    "    \n",
    "So document can be represented as [3,18,84,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Features:\n",
    "\n",
    "- Meta Features: Words, Characters\n",
    "- NLP Features: POS Tags, Grammar Relations\n",
    "- Statistical Features: word/Interaction Frequencies\n",
    "- Word Vector Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix: Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any document can be represented as Document Term Matrix\n",
    "\n",
    "                                    (Rows X Columns = Documents X Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency and Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a very important in Text and Information Mining. It is a better representation of words than normal count vectorizer. Because it measures the relative importance of a word in the corpus. \n",
    "\n",
    "- Instead of filling document matrix by count, we can represent them in TF-IDF Scores\n",
    "- The score will be higher for rare and relavant terms which is opposite to Count vectorization\n",
    "- The score is lower for frequently occuring terms like Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sparsity\n",
    "\n",
    "It is used to describe the phenomenon of not observing enough data in a corpus to model language accurately.\n",
    "\n",
    "\n",
    "Sparse Matrix : A Matrix with many zeros and having very few nonzeros. E.g. In 4X6 Matrix, you see only one value in each row and rest zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips to reduce sparsity\n",
    "\n",
    "1. Text Cleaning (Normalization, Stop Words Removal)\n",
    "2. Matrix Decomposition Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Decomposition Techniques\n",
    "\n",
    "- SVD (Singular Value Matrix Decomposition)\n",
    "\n",
    "    \n",
    "    - The issue associated with SVD is the context of the words in the sentences will not be captured. When we apply SVD to \n",
    "      a Matrix A, it will be decomposed into dense matrix which can be used for classification of regression purpose.\n",
    "    \n",
    "\n",
    "- LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "\n",
    "- NNMF (Non Negative Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Text Classification has been done both with CountVectorizer and TFIDF in Jupyter Notebook \"Count Vector, TFIDF Representations of Text\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorial Representation of Text: Word Vector Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In document terms, every word has a vector and every document has a vector. In other words, every term can be represented as a vector notation. \n",
    "\n",
    "- In this vectors, those numbers will be quantified using counts or TF-IDF scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings: It generate vectors for each word in the corpus. Each word vector represents the position of word in contextual space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word Vector = Context / Meaning + Relationships\n",
    "\n",
    "E.g. \n",
    "\n",
    "1. Word Vector for 'MAN' will be similar to 'UNCLE' & 'KING' \n",
    "2. Word Vector for 'WOMAN' will be similar to 'AUNT' & 'QUEEN'\n",
    "\n",
    "\n",
    "Note: Interestingly the distance b/n MAN and WOMAN will be same as UNCLE => AUNT and KING => QUEEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors can be obtained by using 2 techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training of words from Scratch using Neural Network Architecture on Large Amount of text data\n",
    "      Code:keras.layers.Embedding(input_dim,output_dim,embeddings_initializer='uniform',embeddings_regularizer=None,activity_regularizer=None,embeddings_constraint=None,mask_zero=False,input_length=None)\n",
    "\n",
    "\n",
    "- Pretrained Word Embedding Models\n",
    "\n",
    "    - Word2Vec (Developed by Google)\n",
    "        - It is the combination of two shallow neural networks\n",
    "            1. Continuous bag of words (CBOW) - Predicts the probability of a word given a context\n",
    "            2. Skip-gram Model - Predicts the context from sentence if some words are given\n",
    "            \n",
    "    - Glove (Developed by Stanford)\n",
    "        - It works similar to Word2vec with small differences\n",
    "        - It learns the representations by constructing a co-occurence matrix of words and context which is similar to LSA\n",
    "    \n",
    "    - Fasttext (Developed by Facebook)\n",
    "        - It provides an improvement as it breaks words into several n-grams (Apple => app,ppl, ple)\n",
    "        - The word embedding for Apple will be the sum of all these n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. Fasttext need longer time to train as training happens at character n-gram level\n",
    "2. Need Large memory but performance is better than word2vec and Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All these features can be used in ML and Deep Learning and other information retrieval tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
