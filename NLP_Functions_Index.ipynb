{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cleaning Function\n",
    "\n",
    "- lower case\n",
    "- removal of punctuation\n",
    "- removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords \n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def _clean(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = \"\".join(x for x in txt if x not in string.punctuation)\n",
    "    words = txt.split()\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    txt = \" \".join(words)\n",
    "    return txt\n",
    "\n",
    "tweets[\"cleaned\"] = tweets[\"text\"].apply(lambda x : _clean(x))\n",
    "tweets[[\"text\", \"cleaned\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 'tweets' is a dataframe\n",
    "    - 'text' is a direct column from DF\n",
    "    - 'cleaned' is a derived column after cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fetching Top 100 Repetetive Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keyword Analysis \n",
    "from collections import Counter\n",
    "complete_text = \" \".join(tweets[\"text\"])\n",
    "clean_text = _clean(complete_text)\n",
    "Counter(clean_text.split()).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fetching Top 100 Mentions (Personalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top Mentions \n",
    "mentions = [w for w in complete_text.split() if w.startswith(\"@\")]\n",
    "Counter(mentions).most_common(100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fetching Top 100 Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htags = [w for w in complete_text.split() if w.startswith(\"#\")]\n",
    "htags = [w for w in htags if \"demo\" not in w.lower()]\n",
    "Counter(htags).most_common(100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fetching Top 100 URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htags = [w for w in complete_text.split() if w.startswith(\"http\")]\n",
    "htags = [w for w in htags if \"demon\" not in w.lower()]\n",
    "Counter(htags).most_common(100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fetching Top 100 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "bigrams = ngrams(clean_text.split(), 2)\n",
    "Counter(bigrams).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Extracting Named Enities (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NER \n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag \n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "for text in tweets[\"text\"]:\n",
    "    entities = nltk.ne_chunk(pos_tag(word_tokenize(text))) \n",
    "    for chunk in entities:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            if \"GPE\" in (str(chunk)):\n",
    "                print (chunk)\n",
    "            if \"ORGANIZATION\" in (str(chunk)):\n",
    "                print (chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-written while studying\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "for text in tweets[\"text\"]:\n",
    "    entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    for entity in entities:\n",
    "        if hasattr(entity, \"label\"):\n",
    "            print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic Modelling \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "\n",
    "def generate_topic_models(text):\n",
    "    cvectorizer = CountVectorizer(min_df=4, max_features=2000)\n",
    "    cvz = cvectorizer.fit_transform(text)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=20, random_state=42)\n",
    "    X_topics = lda_model.fit_transform(cvz)\n",
    "\n",
    "    topic_word = lda_model.components_ \n",
    "    vocab = cvectorizer.get_feature_names()\n",
    "    return topic_word, vocab \n",
    "\n",
    "n_top_words = 10\n",
    "topic_word, vocab = generate_topic_models(tweets[\"cleaned\"].values)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print (\"Topic \" + str(i+1) + \": \" + \" | \".join(topic_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Extracting top 30 words visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(x, terms=30):\n",
    "    \n",
    "    # combine all the articles\n",
    "    text =' '.join([text for text in x])\n",
    "    \n",
    "    #split text into words\n",
    "    all_words = text.split()\n",
    "    \n",
    "    #prepare a dictionary of word-frequency pairs\n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),'count':list(fdist.values())})\n",
    "    \n",
    "    #select top n most frequent words\n",
    "    \n",
    "    d = words_df.nlargest(columns = 'count', n = terms)\n",
    "    \n",
    "    #plot the word counts\n",
    "    \n",
    "    plt.figure(figsize = (20,5))\n",
    "    ax = sns.barplot(data = d, x ='word', y = 'count')\n",
    "    \n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words(clean_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
