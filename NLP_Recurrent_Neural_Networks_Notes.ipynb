{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    All though below sentences has same set of words, sequence plays an important role in the context extraction.\n",
    "    \n",
    "           \"working love learning I deep in\"\n",
    "           \"I love working in deep learning\"\n",
    "           \n",
    "    \n",
    "    Real Time Problems:\n",
    "    - Time Series Data\n",
    "    - Video Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics**\n",
    "\n",
    "- Introduction to RNN\n",
    "- Training an RNN Model\n",
    "- Need for LSTM/GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the Sequence of Numbers [12,96,32,45,?], Multilayer Perceptron is not a good idea here as Sequence matters a lot. If the sequence of words misses, classification of Sentiment goes wrong.\n",
    "\n",
    "\n",
    "    The food was good, not bad at all ==> Good Sentiment (Correct - Assumption)\n",
    "    The food was bad, not good at all ==> Bad Sentiment (Incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient\n",
    "\n",
    "Network is unable to learn long term Dependencies.\n",
    "\n",
    "E.g. \n",
    "\n",
    "The **writer** of the books **is** ==> Need to predict what the next word is after ‘books’. Here sentence is small, if Network is able to take ‘Writer’ into account then it understand it as singular and prediction would be is\n",
    "\n",
    "The writer of the **books** are ==> But for some reason, say this sentence is too long, due to the vanishing gradient problem, network is not able to learn the long term dependency ‘writer’ and it is able to look at the nearby dependencies ‘books’. In this case, network gives a wrong prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding Gradient\n",
    "\n",
    "The value of intermediate gradients during propagation are large. When it multiply, it will give large value and crashes the model due to numeric overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanishing Gradient & Expoding Gradient are the two common problems in RNN. They are also common in NN but not as much as common in RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Vanishing_Exploding Gradient.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'Vanishing_Exploding Gradient.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For vanishing gradients, during the weight initialization step i.e. before forward propagation, choose the weights in such a way that the gradients do not diminish. This is quite tricky to achieve. The next stable and simpler solution is advanced RNN Architectures like LSTM/GRU which solves this problem very beautifully.\n",
    "\n",
    "    For Exploding gradients, truncated backpropagation is the one solution. The other solution is Gradient Clipping, every time you get the gradient beyond the threshold value clip it off. LSTM/GRU outperforms in sequence learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of RNN:\n",
    "\n",
    "    - Tries to learn a lot of information quickly\n",
    "    - Not able to remember things in long term (Vanishing Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. \n",
    "\n",
    "Sentence 1:\n",
    "\n",
    "**Bob** is a nice person. **Dan** on the other hand is evil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"LSTM_Gate1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'LSTM_Gate1.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    when it is moving from sentence 1 to sentence 2, network should forget Bob and remember Dan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"LSTM_Gate2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'LSTM_Gate2.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"LSTM.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'LSTM.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In RNN, we will have only ht where as in LSTM it contains both ht and ct which helps to remember long term dependencies. This is how it handles Vanishing Gradient Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "\n",
    "    - Simpler alternative to LSTM (lesser gates) with no seperate cell state (ct) and hence fast in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"GRU.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'GRU.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Although GRU is simpler architecture, it will be able to give all the information whatever LSTM has given. It is very easy and faster to train. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refer Categorization of Websites using LSTM/GRU Project** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Learning the phrases is nothing but a sequence of words. Learn a sequence of one type and convert into another type.\n",
    "    \n",
    "    E.g.\n",
    "    \n",
    "        1. Machine Translation\n",
    "        2. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Machine Translation.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'Machine Translation.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq-2-Seq is built using RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Machine Translation 1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'Machine Translation 1.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq-2-Seq Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Seq-2-Seq.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img = 'Seq-2-Seq.png'\n",
    "Image(url=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Encoder and Decoder are RNN or LSTM Networks. There are 2 phrases in the architecture.\n",
    "    \n",
    "    1. Training Phrase (Encoder)\n",
    "    2. Inference Phrase (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
