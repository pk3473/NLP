{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics\n",
    "\n",
    "- A repeating group of statistically significant tokens or words in a corpus.\n",
    "- Group of frequently occuring words together in the document.\n",
    "- Finds hidden contexts from thg documents using unsupervised techniques.\n",
    "\n",
    "E.g. In a corpus, this helps to group like below\n",
    "\n",
    "Topic 1 – Sports; Topic 2 – Space; Topic 3 – System/Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modelling Techniques**\n",
    "\n",
    "    1. LDA - Latent Dirichlet Allocation\n",
    "    2. NNMF - Non-negative Matrix Factorization\n",
    "    3. LSA - Latent Semantic Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Cases:**\n",
    "\n",
    "    1. Document Categorization\n",
    "    2. Document Summarization\n",
    "    3. Dimensionality Reduction\n",
    "    4. Information Retrieval\n",
    "    5. Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latent Dirichlet Allocation\n",
    "\n",
    "- It identifies most important topics\n",
    "- Finds Topic to Term and Document to Topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "\n",
    "def generate_topic_models(text):\n",
    "    cvectorizer = CountVectorizer(min_df=4, max_features=2000)\n",
    "    cvz = cvectorizer.fit_transform(text)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=20, random_state=42)\n",
    "    X_topics = lda_model.fit_transform(cvz)\n",
    "\n",
    "    topic_word = lda_model.components_ \n",
    "    vocab = cvectorizer.get_feature_names()\n",
    "    return topic_word, vocab \n",
    "\n",
    "n_top_words = 10\n",
    "topic_word, vocab = generate_topic_models(tweets[\"cleaned\"].values)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print (\"Topic \" + str(i+1) + \": \" + \" | \".join(topic_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 'tweets' is a dataframe\n",
    "    - 'text' is a direct column from DF\n",
    "    - 'cleaned' is a derived column after cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Need to convert the doc into numerical representation. Most common numerical representation are counts or TF-IDF's\n",
    "# To compute the counts of each word present in the document, we can use count vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cvectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"i love cooking\", \"I have prepared a cake today\",\"he is going to new place\", \"he will learn cooking there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvz = cvectorizer.fit_transform(corpus)\n",
    "\n",
    "cvz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cake',\n",
       " 'cooking',\n",
       " 'going',\n",
       " 'have',\n",
       " 'he',\n",
       " 'is',\n",
       " 'learn',\n",
       " 'love',\n",
       " 'new',\n",
       " 'place',\n",
       " 'prepared',\n",
       " 'there',\n",
       " 'to',\n",
       " 'today',\n",
       " 'will']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = cvectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33409872, 1.3520179 , 0.33426983, 0.33409872, 0.3344864 ,\n",
       "        0.33426983, 0.33484162, 1.33184251, 0.33426983, 0.33426983,\n",
       "        0.33409872, 0.33484162, 0.33426983, 0.33409872, 0.33484162],\n",
       "       [1.33225166, 0.33510505, 0.33404224, 1.33225166, 0.33407486,\n",
       "        0.33404224, 0.33419528, 0.33426477, 0.33404224, 0.33404224,\n",
       "        1.33225166, 0.33419528, 0.33404224, 1.33225166, 0.33419528],\n",
       "       [0.33364962, 1.31287705, 1.33168793, 0.33364962, 2.33143874,\n",
       "        1.33168793, 1.33096309, 0.33389272, 1.33168793, 1.33168793,\n",
       "        0.33364962, 1.33096309, 1.33168793, 0.33364962, 1.33096309]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing LDA model, this cannot run on text form\n",
    "\n",
    "# n_components topics to be extracted, max_iter = no of iterations to run before finding the optimal representation of topics and \n",
    "# document topics\n",
    "# random_state - represents seed, useful for reproducibility\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 3, max_iter = 20,random_state=20)\n",
    "X_topics = lda_model.fit_transform(cvz) \n",
    "topic_words = lda_model.components_ # gives topic distribution\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, LDA model is applied on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['cooking' 'love' 'will']\n",
      "Topic 2 ['today' 'prepared' 'have']\n",
      "Topic 3 ['he' 'to' 'place']\n"
     ]
    }
   ],
   "source": [
    "# Viewing the obtained topics\n",
    "\n",
    "# Print the number of words present in every topic\n",
    "\n",
    "n_top_words = 4\n",
    "\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    sorted_topic_dist = np.argsort(topic_dist) # \"argsort\" is useful to sort an vector/array/list/metrics\n",
    "    topic_words = np.array(vocab)[sorted_topic_dist]\n",
    "    #To view the actual words present in the index. Array conversion helps to access the element by index\n",
    "    topic_words = topic_words[:-n_top_words:-1] # -n_top_words = reversing the sign of n_top_words\n",
    "    print(\"Topic\",str(i+1),topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these are not accurate, when we have more amount of data and run for large number of iterations then we often obtain very good representation of topics and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Topic:--- 0\n",
      "Document 2 Topic:--- 1\n",
      "Document 3 Topic:--- 2\n",
      "Document 4 Topic:--- 2\n"
     ]
    }
   ],
   "source": [
    "# Viewing the topics assigned to each document\n",
    "\n",
    "doc_topic = lda_model.transform(cvz)\n",
    "for n in range (doc_topic.shape[0]):# Iterating in every possible value of this document till its end. shape[0] represents number of rows in the document\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    print(\"Document\",n+1,\"Topic:---\",topic_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Libraries such as Gensem and SpaCy also can be used to implement Topic Modeling and LDA Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Latent Semantic Analysis**\n",
    "\n",
    "While working with unstructured data, to extract useful information two types of problems are very common. \n",
    "\n",
    "- Synonymy\n",
    "    -Same concept but different terms. \n",
    "    - E.g. Laborer, Worker, Employee, Workman etc.\n",
    "- Ploysemy\n",
    "    - Identical terms but different contexts\n",
    "    - E.g. \"book\": A reading \"book\" or \"Book\" the Tickets\n",
    "    \n",
    "\n",
    "Steps:\n",
    "    1. Represents the corpus as the Document term matrix\n",
    "    2. Decomposes into Document Topic and Topic Term matrix using SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refer to Jupyter Notebook: Topic_Modelling_BBC_Articles for LSA Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non Negative Matrix Factorization\n",
    "\n",
    "- This technique is a decomposition of matrix by considering that Negative Numbers of use-less\n",
    "- Transforms a high dimensional matrix (D) into two low dimensional non-negative matrices W & H\n",
    "\n",
    "    D- Doument Term Matrix\n",
    "    W - Basis Vectors: Topics obtained from the documents\n",
    "    H - Coefficient Matrix: Weights correspoding to every topic in the document\n",
    "\n",
    "The goal of this technique is to find the optimal values W and H using 3 steps\n",
    "\n",
    "    1. Cost Function: Measure of Error\n",
    "    2. Optimization: Minimization of Error\n",
    "    3. Updating Rules: update W and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 20 Newsgroup dataset from sklearn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle = True, random_state =1, remove = ('headers','footers','quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset has 11314 text documents distributed across 20 different newsgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document': documents})\n",
    "\n",
    "#removing everything except alphabets\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "#removing short words\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#make all text lowercase\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) \n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix\n",
    "\n",
    "This is the first step towards topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000, # keep top 1000 terms \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# number of topics\n",
    "num_topics = 20\n",
    "\n",
    "nmf = NMF(n_components=num_topics, random_state=1, \n",
    "          alpha=.1, l1_ratio=.5, init='nndsvd').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display and Evaluate Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic \"+str(topic_idx)+\": \")\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "right believe make really said point want jesus things question\n",
      "Topic 1: \n",
      "thanks mail advance looking info help information address email appreciated\n",
      "Topic 2: \n",
      "game team year games season players play hockey league teams\n",
      "Topic 3: \n",
      "drive scsi hard drives disk floppy controller internal tape cable\n",
      "Topic 4: \n",
      "windows file files program version directory using running software graphics\n",
      "Topic 5: \n",
      "chip clipper encryption government keys phone data escrow algorithm chips\n",
      "Topic 6: \n",
      "like sounds looks look sound things thing make sure really\n",
      "Topic 7: \n",
      "card video monitor cards drivers driver color memory board mode\n",
      "Topic 8: \n",
      "know anybody want need program appreciated sure maybe help getting\n",
      "Topic 9: \n",
      "people government rights person world country force guns society life\n",
      "Topic 10: \n",
      "think wrong really pretty steve remember makes wait agree original\n",
      "Topic 11: \n",
      "problem problems using apple screen fine error work worked solution\n",
      "Topic 12: \n",
      "good thing pretty better year looking quality world idea chance\n",
      "Topic 13: \n",
      "space nasa shuttle launch orbit station moon earth lunar data\n",
      "Topic 14: \n",
      "sale offer price condition shipping email asking interested best sell\n",
      "Topic 15: \n",
      "israel israeli jews arab jewish peace state land anti killed\n",
      "Topic 16: \n",
      "bike miles engine road fast turn left right insurance advice\n",
      "Topic 17: \n",
      "time long years access running goes mode took remember finally\n",
      "Topic 18: \n",
      "armenian armenians turkish armenia turkey turks genocide said government soviet\n",
      "Topic 19: \n",
      "window application display manager server motif using widget program code\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "display_topics(nmf, X_feature_names, top_n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
