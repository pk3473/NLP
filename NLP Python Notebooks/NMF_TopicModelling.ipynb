{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling using Non Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading and inspection\n",
    "\n",
    "Let's load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 20 Newsgroup dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 11,314 text documents distributed across 20 different newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "to start with, we will try to clean our text data as much as possible. We will remove the punctuations, numbers, special characters all in one step using the regex replace(\"[^a-zA-Z#]\", \" \") which will replace everything, except alphabets, with a space. Then we will remove shorter words because usually they don't contain much of the information. Finally we will make all the text lowercase to nullify case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# removing everything except alphabets`\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) \n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix\n",
    "\n",
    "This is the first step towards topic modeling. We will use sklearn's TfidfVectorizer to create a document-term matrix with 1000 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000, # keep top 1000 terms \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# number of topics\n",
    "num_topics = 20\n",
    "\n",
    "nmf = NMF(n_components=num_topics, random_state=1, \n",
    "          alpha=.1, l1_ratio=.5, init='nndsvd').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and Evaluate Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic \"+str(topic_idx)+\": \")\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "right believe make really said point want jesus things question\n",
      "Topic 1: \n",
      "thanks mail advance looking info help information address email appreciated\n",
      "Topic 2: \n",
      "game team year games season players play hockey league teams\n",
      "Topic 3: \n",
      "drive scsi hard drives disk floppy controller internal tape cable\n",
      "Topic 4: \n",
      "windows file files program version directory using running software graphics\n",
      "Topic 5: \n",
      "chip clipper encryption government keys phone data escrow algorithm chips\n",
      "Topic 6: \n",
      "like sounds looks look sound things thing make sure really\n",
      "Topic 7: \n",
      "card video monitor cards drivers driver color memory board mode\n",
      "Topic 8: \n",
      "know anybody want need program appreciated sure maybe help getting\n",
      "Topic 9: \n",
      "people government rights person world country force guns society life\n",
      "Topic 10: \n",
      "think wrong really pretty steve remember makes wait agree original\n",
      "Topic 11: \n",
      "problem problems using apple screen fine error work worked solution\n",
      "Topic 12: \n",
      "good thing pretty better year looking quality world idea chance\n",
      "Topic 13: \n",
      "space nasa shuttle launch orbit station moon earth lunar data\n",
      "Topic 14: \n",
      "sale offer price condition shipping email asking interested best sell\n",
      "Topic 15: \n",
      "israel israeli jews arab jewish peace state land anti killed\n",
      "Topic 16: \n",
      "bike miles engine road fast turn left right insurance advice\n",
      "Topic 17: \n",
      "time long years access running goes mode took remember finally\n",
      "Topic 18: \n",
      "armenian armenians turkish armenia turkey turks genocide said government soviet\n",
      "Topic 19: \n",
      "window application display manager server motif using widget program code\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "display_topics(nmf, X_feature_names, top_n_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
