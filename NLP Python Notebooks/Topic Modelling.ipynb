{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Need to convert the doc into numerical representation. Most common numerical representation are counts or TF-IDF's\n",
    "# To compute the counts of each word present in the document, we can use count vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cvectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"i love cooking\", \"I have prepared a cake today\",\"he is going to new place\", \"he will learn cooking there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvz = cvectorizer.fit_transform(corpus)\n",
    "\n",
    "cvz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Rows and 15 Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cake',\n",
       " 'cooking',\n",
       " 'going',\n",
       " 'have',\n",
       " 'he',\n",
       " 'is',\n",
       " 'learn',\n",
       " 'love',\n",
       " 'new',\n",
       " 'place',\n",
       " 'prepared',\n",
       " 'there',\n",
       " 'to',\n",
       " 'today',\n",
       " 'will']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = cvectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33409872, 1.3520179 , 0.33426983, 0.33409872, 0.3344864 ,\n",
       "        0.33426983, 0.33484162, 1.33184251, 0.33426983, 0.33426983,\n",
       "        0.33409872, 0.33484162, 0.33426983, 0.33409872, 0.33484162],\n",
       "       [1.33225166, 0.33510505, 0.33404224, 1.33225166, 0.33407486,\n",
       "        0.33404224, 0.33419528, 0.33426477, 0.33404224, 0.33404224,\n",
       "        1.33225166, 0.33419528, 0.33404224, 1.33225166, 0.33419528],\n",
       "       [0.33364962, 1.31287705, 1.33168793, 0.33364962, 2.33143874,\n",
       "        1.33168793, 1.33096309, 0.33389272, 1.33168793, 1.33168793,\n",
       "        0.33364962, 1.33096309, 1.33168793, 0.33364962, 1.33096309]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing LDA model, this cannot run on text form\n",
    "\n",
    "# n_components topics to be extracted, max_iter = no of iterations to run before finding the optimal representation of topics and \n",
    "# document topics\n",
    "# random_state - represents seed, useful for reproducibility\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 3, max_iter = 20,random_state=20)\n",
    "X_topics = lda_model.fit_transform(cvz) \n",
    "topic_words = lda_model.components_ # gives topic distribution\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, LDA model is applied on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['cooking' 'love' 'will']\n",
      "Topic 2 ['today' 'prepared' 'have']\n",
      "Topic 3 ['he' 'to' 'place']\n"
     ]
    }
   ],
   "source": [
    "# Viewing the obtained topics\n",
    "\n",
    "# Print the number of words present in every topic\n",
    "\n",
    "n_top_words = 4\n",
    "\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    sorted_topic_dist = np.argsort(topic_dist) # \"argsort\" is useful to sort an vector/array/list/metrics\n",
    "    topic_words = np.array(vocab)[sorted_topic_dist]\n",
    "    #To view the actual words present in the index. Array conversion helps to access the element by index\n",
    "    topic_words = topic_words[:-n_top_words:-1] # -n_top_words = reversing the sign of n_top_words\n",
    "    print(\"Topic\",str(i+1),topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these are not accurate, when we have more amount of data and run for large number of iterations then we often obtain very good representation of topics and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Topic:--- 0\n",
      "Document 2 Topic:--- 1\n",
      "Document 3 Topic:--- 2\n",
      "Document 4 Topic:--- 2\n"
     ]
    }
   ],
   "source": [
    "# Viewing the topics assigned to each document\n",
    "\n",
    "doc_topic = lda_model.transform(cvz)\n",
    "for n in range (doc_topic.shape[0]):# Iterating in every possible value of this document till its end. shape[0] represents number of rows in the document\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    print(\"Document\",n+1,\"Topic:---\",topic_doc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Libraries such as Gensem and SpaCy also can be used to implement Topic Modeling and LDA Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using NNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA is not available. Search in free time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 20 Newsgroup dataset from sklearn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle = True, random_state =1, remove = ('headers','footers','quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 11314 text documents distributed across 20 different newsgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document': documents})\n",
    "\n",
    "#removing everything except alphabets\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "#removing short words\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#make all text lowercase\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) \n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first step towards topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000, # keep top 1000 terms \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# number of topics\n",
    "num_topics = 20\n",
    "\n",
    "nmf = NMF(n_components=num_topics, random_state=1, \n",
    "          alpha=.1, l1_ratio=.5, init='nndsvd').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and Evaluate Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic \"+str(topic_idx)+\": \")\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "right believe make really said point want jesus things question\n",
      "Topic 1: \n",
      "thanks mail advance looking info help information address email appreciated\n",
      "Topic 2: \n",
      "game team year games season players play hockey league teams\n",
      "Topic 3: \n",
      "drive scsi hard drives disk floppy controller internal tape cable\n",
      "Topic 4: \n",
      "windows file files program version directory using running software graphics\n",
      "Topic 5: \n",
      "chip clipper encryption government keys phone data escrow algorithm chips\n",
      "Topic 6: \n",
      "like sounds looks look sound things thing make sure really\n",
      "Topic 7: \n",
      "card video monitor cards drivers driver color memory board mode\n",
      "Topic 8: \n",
      "know anybody want need program appreciated sure maybe help getting\n",
      "Topic 9: \n",
      "people government rights person world country force guns society life\n",
      "Topic 10: \n",
      "think wrong really pretty steve remember makes wait agree original\n",
      "Topic 11: \n",
      "problem problems using apple screen fine error work worked solution\n",
      "Topic 12: \n",
      "good thing pretty better year looking quality world idea chance\n",
      "Topic 13: \n",
      "space nasa shuttle launch orbit station moon earth lunar data\n",
      "Topic 14: \n",
      "sale offer price condition shipping email asking interested best sell\n",
      "Topic 15: \n",
      "israel israeli jews arab jewish peace state land anti killed\n",
      "Topic 16: \n",
      "bike miles engine road fast turn left right insurance advice\n",
      "Topic 17: \n",
      "time long years access running goes mode took remember finally\n",
      "Topic 18: \n",
      "armenian armenians turkish armenia turkey turks genocide said government soviet\n",
      "Topic 19: \n",
      "window application display manager server motif using widget program code\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "display_topics(nmf, X_feature_names, top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
