{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation helps to make the model more generalizable when there is less data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Computer Vision, it generates more images with the help of Image Rotation, Changing Lighting Conditions, Image Cropping etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Text Augmentation, it generates more text data which is a cost effective in terms of money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I visited this place on Valentine's day in the evening. We had to wait for around 15 minutes to get a table here. We utilised that time in getting pictures ;) This place had a very chill vibe with loud music in the background. Live telecast of a cricket match was also going on. It was a little difficult to hold a conversation with someone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I visited this place on Valentine's day in the evening. We had to wait for around 15 minutes to get a table here. We utilised that time in getting pictures ;) This place had a very chill vibe with loud music in the background. Live telecast of a cricket match was also going on. It was a little difficult to hold a conversation with someone."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Technique: Take off words randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Tokenize the sample text\n",
    "2. Find the number of tokens\n",
    "3. Set the percentage of words to keep\n",
    "4. Randomly omit the words to generate new text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenize the sample text \n",
    "\n",
    "tokens = []\n",
    "for token in doc:\n",
    "    tokens.append(token.text)\n",
    "    \n",
    "# find the number of tokens\n",
    "l = len(tokens)\n",
    "\n",
    "# number of tokens to keep\n",
    "random_tokens_count = round(0.8*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store augmented text\n",
    "augmented_texts = []\n",
    "\n",
    "# number of versions of augmented text\n",
    "n = 5\n",
    "\n",
    "for i in range(n):\n",
    "    # randomly generate indices of tokens to keep\n",
    "    new_tokens_index = random.sample(range(l), random_tokens_count)\n",
    "    new_tokens_index.sort()\n",
    "    \n",
    "    # generate augmented list of tokens\n",
    "    new_tokens = [tokens[t] for t in new_tokens_index]\n",
    "    \n",
    "    # create of augmented version of sample_text\n",
    "    augmented_texts.append(' '.join(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this place on Valentine day in the evening . had to for around 15 get a table here . We utilised time getting pictures ;) This place had a very chill with loud music in background . Live telecast of cricket match was also going on . It was a little difficult hold a conversation with someone\n",
      "\n",
      "I visited this place on Valentine 's day in the evening . We to wait for 15 get table here We utilised that time in getting pictures ;) This place had very vibe with loud music the background . Live of a cricket match going . It was a little difficult to hold a conversation with someone\n",
      "\n",
      "I visited this on day in the evening . had to wait for around 15 minutes to get a table We utilised time in getting pictures ;) This place had a very chill vibe loud music in the background . telecast of a cricket match was also going on . was a difficult hold a conversation with\n",
      "\n",
      "I visited place on in the evening . had to wait for around 15 to get a table . We utilised that time in getting pictures ;) This place a very vibe with music in the . Live telecast of a cricket match was also going on It was a little to hold a conversation with .\n",
      "\n",
      "I visited on 's day the evening . We had to wait for around 15 minutes to get a table here We utilised time in getting pictures ;) This had very vibe with loud music the background . telecast of a cricket match was also going on . It was a difficult to hold a with .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print augmented texts\n",
    "for i in augmented_texts:\n",
    "    print(i + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Concatenate POS Tags to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sample_text\n",
    "tokens_pos = []\n",
    "for token in doc:\n",
    "    tokens_pos.append(token.text+\"_\"+token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I_PRON', 'visited_VERB', 'this_DET', 'place_NOUN', 'on_ADP', 'Valentine_PROPN', \"'s_PART\", 'day_NOUN', 'in_ADP', 'the_DET', 'evening_NOUN', '._PUNCT', 'We_PRON', 'had_VERB', 'to_PART', 'wait_VERB', 'for_ADP', 'around_ADP', '15_NUM', 'minutes_NOUN', 'to_PART', 'get_VERB', 'a_DET', 'table_NOUN', 'here_ADV', '._PUNCT', 'We_PRON', 'utilised_VERB', 'that_DET', 'time_NOUN', 'in_ADP', 'getting_VERB', 'pictures_NOUN', ';)_PUNCT', 'This_DET', 'place_NOUN', 'had_VERB', 'a_DET', 'very_ADV', 'chill_ADJ', 'vibe_NOUN', 'with_ADP', 'loud_ADJ', 'music_NOUN', 'in_ADP', 'the_DET', 'background_NOUN', '._PUNCT', 'Live_ADJ', 'telecast_NOUN', 'of_ADP', 'a_DET', 'cricket_NOUN', 'match_NOUN', 'was_VERB', 'also_ADV', 'going_VERB', 'on_PART', '._PUNCT', 'It_PRON', 'was_VERB', 'a_DET', 'little_ADJ', 'difficult_ADJ', 'to_PART', 'hold_VERB', 'a_DET', 'conversation_NOUN', 'with_ADP', 'someone_NOUN', '._PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_PRON visited_VERB this_DET place_NOUN on_ADP Valentine_PROPN 's_PART day_NOUN in_ADP the_DET evening_NOUN ._PUNCT We_PRON had_VERB to_PART wait_VERB for_ADP around_ADP 15_NUM minutes_NOUN to_PART get_VERB a_DET table_NOUN here_ADV ._PUNCT We_PRON utilised_VERB that_DET time_NOUN in_ADP getting_VERB pictures_NOUN ;)_PUNCT This_DET place_NOUN had_VERB a_DET very_ADV chill_ADJ vibe_NOUN with_ADP loud_ADJ music_NOUN in_ADP the_DET background_NOUN ._PUNCT Live_ADJ telecast_NOUN of_ADP a_DET cricket_NOUN match_NOUN was_VERB also_ADV going_VERB on_PART ._PUNCT It_PRON was_VERB a_DET little_ADJ difficult_ADJ to_PART hold_VERB a_DET conversation_NOUN with_ADP someone_NOUN ._PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(tokens_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Replace Named Entities with their Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited this place on PERSON's day in the evening. We had to wait for TIME to get a table here. We utilised that time in getting pictures ;) This place had a very chill vibe with loud music in the background. Live telecast of a cricket match was also going on. It was a little difficult to hold a conversation with someone.\n"
     ]
    }
   ],
   "source": [
    "aug_text = sample_text\n",
    "for ent in doc.ents:\n",
    "    aug_text = re.sub(ent.text, ent.label_, aug_text)\n",
    "    \n",
    "print(aug_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workout on a challenge downloading the data from www.yelp.com/dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
