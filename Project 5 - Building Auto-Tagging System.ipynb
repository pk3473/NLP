{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module, we have solved text classification problem with both ML (Logistic, Random Forest, XGBoost etc) and Deep Learning Techniques (CNN, RNN & LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement: Building Automatic Question Tagging system on Stackoverflow dataset\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the Module\n",
    "\n",
    "    1. Understand the Business Problem\n",
    "    2. Business Problem into Datascience Problem\n",
    "    3. About the Dataset\n",
    "    4. Performance Metrics\n",
    "    5. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stackover flow is an online platform where people ask questions related to Computer Science. These questions can be upvoted or downvoted by users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Title: \n",
    "    \n",
    "    Description:\n",
    "    \n",
    "        Tags:  This acts as indicators of the topics which this question is covering. This tags are annotated by users or \n",
    "               predicted by predictive models on stackover flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we will study how these tags are predicted automatically. Input will be the Title and description of the question which are text. We extract features from these text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Automatic Tagging makes business sense?\n",
    "\n",
    "   In Stackoverflow, thousands of questions will be asked and it is not an easy task for the experts to find the answer related to the question for their domain experties. Hence correctly tagging is very important and accurate tagging is profitable for the business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    . Over 76,000 data science related questions\n",
    "    . 100 unique tags (Multiple Questions might have same tag)\n",
    "    . Maximum 5 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    . High Precision: Out of all the predicted tags, how many actually belong to that question\n",
    "    \n",
    "    . High recall: Out of all the actual tags, how many show up in the predictions\n",
    "    \n",
    "    . Performance Metric: F1 Score. It gives good value if both precision and recall are high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "    1. Import Libraries and load datasets\n",
    "    2. Inspect data\n",
    "    3. clean and pre-process data\n",
    "    4. reshape the target variable\n",
    "    5. extract features from the text \n",
    "    6. Build multilabel classification model\n",
    "    7. make predictions and evaluate model\n",
    "    8. define inference function for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm # it is handly library which provide percentage progress bar while executing for loops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_hdf('auto_tagging_data_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is in hdf.h5 format which is an efficient way to store large amount of data, we are using read.hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learning?</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=\"http://anyall.org/\"&gt;Brendan O'Connor&lt;/a&gt; entitled &lt;a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\"&gt;\"Statistics vs. Mach...</td>\n",
       "      <td>[machine-learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>&lt;p&gt;What are some of the ways to forecast demographic census with some validation and calibration techniques?&lt;/p&gt;\\n\\n&lt;p&gt;Some of the concerns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Census blocks vary in sizes as rural\\n...</td>\n",
       "      <td>[forecasting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain English</td>\n",
       "      <td>&lt;p&gt;How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?&lt;/p&gt;\\n</td>\n",
       "      <td>[bayesian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>What is the meaning of p values and t values in statistical tests?</td>\n",
       "      <td>&lt;p&gt;After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....</td>\n",
       "      <td>[hypothesis-testing, t-test, p-value, interpretation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>Examples for teaching: Correlation does not mean causation</td>\n",
       "      <td>&lt;p&gt;There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;number of storks and birth ...</td>\n",
       "      <td>[correlation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                                               Title  \\\n",
       "0   6                  The Two Cultures: statistics vs. machine learning?   \n",
       "1  21                                      Forecasting demographic census   \n",
       "2  22                 Bayesian and frequentist reasoning in plain English   \n",
       "3  31  What is the meaning of p values and t values in statistical tests?   \n",
       "4  36          Examples for teaching: Correlation does not mean causation   \n",
       "\n",
       "                                                                                                                                                                                                      Body  \\\n",
       "0  <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Mach...   \n",
       "1  <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census blocks vary in sizes as rural\\n...   \n",
       "2                                                                               <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n   \n",
       "3  <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests....   \n",
       "4  <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:</p>\\n\\n<ol>\\n<li>number of storks and birth ...   \n",
       "\n",
       "                                                    Tags  \n",
       "0                                     [machine-learning]  \n",
       "1                                          [forecasting]  \n",
       "2                                             [bayesian]  \n",
       "3  [hypothesis-testing, t-test, p-value, interpretation]  \n",
       "4                                          [correlation]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41763</td>\n",
       "      <td>92185</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?</td>\n",
       "      <td>&lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Approximate a target distribution $p$ using an importance sample $S$ fro...</td>\n",
       "      <td>[sampling, mcmc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4245</td>\n",
       "      <td>179778</td>\n",
       "      <td>optimization approach in logistic regression</td>\n",
       "      <td>&lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...</td>\n",
       "      <td>[machine-learning, logistic, classification, optimization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37183</td>\n",
       "      <td>168679</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model</td>\n",
       "      <td>&lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...</td>\n",
       "      <td>[regression, survival, cox-model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55932</td>\n",
       "      <td>144226</td>\n",
       "      <td>Moments and density tails</td>\n",
       "      <td>&lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does there exist a methodology to...</td>\n",
       "      <td>[probability, pdf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47629</td>\n",
       "      <td>142745</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables?</td>\n",
       "      <td>&lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.&lt;/p&gt;\\n</td>\n",
       "      <td>[variance, covariance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49639</td>\n",
       "      <td>195347</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?</td>\n",
       "      <td>&lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...</td>\n",
       "      <td>[machine-learning, nonlinear-regression]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  \\\n",
       "41763   92185   \n",
       "4245   179778   \n",
       "37183  168679   \n",
       "55932  144226   \n",
       "47629  142745   \n",
       "49639  195347   \n",
       "\n",
       "                                                                                                   Title  \\\n",
       "41763                  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?   \n",
       "4245                                                        optimization approach in logistic regression   \n",
       "37183                             Consequences of violating proportional hazards assumption in Cox model   \n",
       "55932                                                                          Moments and density tails   \n",
       "47629            What is the demonstration of the variance of the difference of two dependent variables?   \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?   \n",
       "\n",
       "                                                                                                                                                                                                          Body  \\\n",
       "41763  <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n<ol>\\n<li>Approximate a target distribution $p$ using an importance sample $S$ fro...   \n",
       "4245   <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...   \n",
       "37183  <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...   \n",
       "55932  <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does there exist a methodology to...   \n",
       "47629                <p>I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.</p>\\n   \n",
       "49639  <p>I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...   \n",
       "\n",
       "                                                             Tags  \n",
       "41763                                            [sampling, mcmc]  \n",
       "4245   [machine-learning, logistic, classification, optimization]  \n",
       "37183                           [regression, survival, cox-model]  \n",
       "55932                                          [probability, pdf]  \n",
       "47629                                      [variance, covariance]  \n",
       "49639                    [machine-learning, nonlinear-regression]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.sample(6,random_state = 11) # random state allows to reproduce the results again and again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't use random state, it gives results at random.\n",
    "\n",
    "Since we need both Title and Body while extracting the features, we will combine this and keep it as a single column 'Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Text'] = df_questions['Title'] + \" \" + df_questions['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41763</td>\n",
       "      <td>92185</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?</td>\n",
       "      <td>&lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Approximate a target distribution $p$ using an importance sample $S$ fro...</td>\n",
       "      <td>[sampling, mcmc]</td>\n",
       "      <td>Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)? &lt;p&gt;From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:&lt;/p&gt;\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4245</td>\n",
       "      <td>179778</td>\n",
       "      <td>optimization approach in logistic regression</td>\n",
       "      <td>&lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...</td>\n",
       "      <td>[machine-learning, logistic, classification, optimization]</td>\n",
       "      <td>optimization approach in logistic regression &lt;p&gt;In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37183</td>\n",
       "      <td>168679</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model</td>\n",
       "      <td>&lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...</td>\n",
       "      <td>[regression, survival, cox-model]</td>\n",
       "      <td>Consequences of violating proportional hazards assumption in Cox model &lt;p&gt;What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55932</td>\n",
       "      <td>144226</td>\n",
       "      <td>Moments and density tails</td>\n",
       "      <td>&lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does there exist a methodology to...</td>\n",
       "      <td>[probability, pdf]</td>\n",
       "      <td>Moments and density tails &lt;p&gt;Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. &lt;/p&gt;\\n\\n&lt;p&gt;Does th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47629</td>\n",
       "      <td>142745</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables?</td>\n",
       "      <td>&lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.&lt;/p&gt;\\n</td>\n",
       "      <td>[variance, covariance]</td>\n",
       "      <td>What is the demonstration of the variance of the difference of two dependent variables? &lt;p&gt;I know that the variance of the difference of two independent variables is the sum of variances, and I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49639</td>\n",
       "      <td>195347</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?</td>\n",
       "      <td>&lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...</td>\n",
       "      <td>[machine-learning, nonlinear-regression]</td>\n",
       "      <td>Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model? &lt;p&gt;I was trying to understand how much data I would need compared to the number of parameters (and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  \\\n",
       "41763   92185   \n",
       "4245   179778   \n",
       "37183  168679   \n",
       "55932  144226   \n",
       "47629  142745   \n",
       "49639  195347   \n",
       "\n",
       "                                                                                                   Title  \\\n",
       "41763                  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)?   \n",
       "4245                                                        optimization approach in logistic regression   \n",
       "37183                             Consequences of violating proportional hazards assumption in Cox model   \n",
       "55932                                                                          Moments and density tails   \n",
       "47629            What is the demonstration of the variance of the difference of two dependent variables?   \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model?   \n",
       "\n",
       "                                                                                                                                                                                                          Body  \\\n",
       "41763  <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n<ol>\\n<li>Approximate a target distribution $p$ using an importance sample $S$ fro...   \n",
       "4245   <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We normally use gradient descent approach there. ...   \n",
       "37183  <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two factors are highly significative, but all the estimated betas associated to...   \n",
       "55932  <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does there exist a methodology to...   \n",
       "47629                <p>I know that the variance of the difference of two independent variables is the sum of variances, and I can prove it. I want to know where the covariance goes in the other case.</p>\\n   \n",
       "49639  <p>I was trying to understand how much data I would need compared to the number of parameters (and to have good generalization) when I train a radial basis function (RBF) network on a regression t...   \n",
       "\n",
       "                                                             Tags  \\\n",
       "41763                                            [sampling, mcmc]   \n",
       "4245   [machine-learning, logistic, classification, optimization]   \n",
       "37183                           [regression, survival, cox-model]   \n",
       "55932                                          [probability, pdf]   \n",
       "47629                                      [variance, covariance]   \n",
       "49639                    [machine-learning, nonlinear-regression]   \n",
       "\n",
       "                                                                                                                                                                                                          Text  \n",
       "41763  Why is Sampling Importance Resampling (SIR) better than Importance Sampling (IS)? <p>From what I understand,  SIR is a mechanism for sampling from a distribution $p$ that works as follows:</p>\\n\\n...  \n",
       "4245   optimization approach in logistic regression <p>In logistic regression we need to maximise the log likelihood which boils down to minimising a function which is sum of multiple log functions. We n...  \n",
       "37183  Consequences of violating proportional hazards assumption in Cox model <p>What are the consequences of violating the Proportional Hazards assumption in a Cox Model? I've got a Model where two fact...  \n",
       "55932  Moments and density tails <p>Assume that the first $n$ moments $m_1,\\dots\\,m_n$ of a random variable $X\\in\\mathbb{R}$ are known, but not its probability density function $p(x)$. </p>\\n\\n<p>Does th...  \n",
       "47629  What is the demonstration of the variance of the difference of two dependent variables? <p>I know that the variance of the difference of two independent variables is the sum of variances, and I ca...  \n",
       "49639  Rules for choosing how much training data one needs to learn a Radial Basis Function (RBF) model? <p>I was trying to understand how much data I would need compared to the number of parameters (and...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.sample(6,random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Two Cultures: statistics vs. machine learning? <p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statisti...\n",
       "1    Forecasting demographic census <p>What are some of the ways to forecast demographic census with some validation and calibration techniques?</p>\\n\\n<p>Some of the concerns:</p>\\n\\n<ul>\\n<li>Census ...\n",
       "2                             Bayesian and frequentist reasoning in plain English <p>How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?</p>\\n\n",
       "3    What is the meaning of p values and t values in statistical tests? <p>After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk b...\n",
       "4    Examples for teaching: Correlation does not mean causation <p>There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustra...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Html tags and urls helps, because it doesn't help in taging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = re.sub(r'<.*?>', '', text) # this removes html tags and url links in the text\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) # this will remove everything except alphabets\n",
    "    \n",
    "    text = ' '.join(text.split()) # # this will remove extra or white spaces in the text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function on text variable\n",
    "\n",
    "df_questions['Text'] = df_questions['Text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Text'] = df_questions['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the two cultures statistics vs machine learning last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the ...\n",
       "1    forecasting demographic census what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural ar...\n",
       "2                                       bayesian and frequentist reasoning in plain english how would you describe in plain english the characteristics that distinguish bayesian from frequentist reasoning\n",
       "3    what is the meaning of p values and t values in statistical tests after taking a statistics course and then trying to help fellow students i noticed one subject that inspires much head desk bangin...\n",
       "4    examples for teaching correlation does not mean causation there is an old saying correlation does not mean causation when i teach i tend to use the following standard examples to illustrate this p...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text looks much cleaner, however there are stopwords. Because they hardly helps in determing the tag of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_stopwords(text):\n",
    "    \n",
    "    # splitting the text into individual words and going through word by word and checking if it is a stop word\n",
    "    \n",
    "    clean_text = [w for w in text.split() if not w in stop_words] \n",
    "    \n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['Text_clean'] = df_questions['Text'].apply(lambda x: strip_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also apply various other text cleaning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the target variable into one hot encoded format, there is transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "\n",
    "multilabel_binarizer.fit(df_questions['Tags'])\n",
    "\n",
    "# transform target variable (\"Tags\")\n",
    "Y = multilabel_binarizer.transform(df_questions['Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76365, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It became an array of 100 Columns (100 Unique tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Step: Feature Extraction from text and train multiple to learn the relationship with newly created 100 target variable\n",
    "\n",
    "# Here they have Tfidf to extract features from text, we can also use other methods such as bag of words, word2vec and glove\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,000 most frequent words, 0.8 = words that appear more than 80% in the document will be discarded\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000) \n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_questions['Text_clean'])\n",
    "\n",
    "# max_df and max_features are hyperparameters, we can change these values and check which values are working for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<76365x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4017592 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.8, max_features=10000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ttrain - test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_tfidf, x_val_tfidf, y_train_tfidf, y_val_tfidf = train_test_split(X_tfidf, Y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<61092x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3224103 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15273x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 793489 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be training as many unique tags (in our case 100 different models). Training 100 Models will be quite time consuming, hence we are taking a simple classifier (LogisticRegression) to build these 100 models. We manually use this by a loop or a better option i.e. OneVsRestClassifier. It will train 100 different Logistic Regression models for each and every tag in out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary Relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "clf = OneVsRestClassifier(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chalampp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\chalampp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\chalampp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model on train data\n",
    "clf.fit(x_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is trained on Train Dataset, we can make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for validation set\n",
    "y_pred = clf.predict(x_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing few prediction\n",
    "\n",
    "# these predictions are one hot encoded format. It is not quite interpretable and difficult to tell which tags our model has predicted\n",
    "\n",
    "print(y_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert these predictions into our tags. We can reuse the object MultiLabelBinarizer and use its inverse transform to convert these predictions into tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prediction',), ('distributions', 'mean', 'variance'), ('r',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_binarizer.inverse_transform(y_pred)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First question Tag: 1 (prediction)\n",
    "\n",
    "Second Question Tag: 3 ('distributions', 'mean', 'variance')\n",
    "\n",
    "Third Question Tag: 1 ('r',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us check what are the actual tags of these questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('confidence-interval', 'regression'),\n",
       " ('distributions', 'mean', 'variance'),\n",
       " ('bayesian', 'r')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_binarizer.inverse_transform(y_val_tfidf[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is missed with First question, second is more accurate where third is 50% correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score between 0 and 1. The best value is at 1 with high precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-4d85a233f608>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1045\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m                        zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1173\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m                                                  zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1176\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1432\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1434\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1263\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m   1264\u001b[0m                              \u001b[1;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m                              % (y_type, average_options))\n\u001b[0m\u001b[0;32m   1266\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples']."
     ]
    }
   ],
   "source": [
    "# evaluate performance\n",
    "f1_score(y_val_tfidf, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error means there is an average parameter in F1 Score which is default set to 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04040404, 0.62266501, 0.63430421, 0.41269841, 0.57627119,\n",
       "       0.15151515, 0.359375  , 0.688     , 0.5326087 , 0.23913043,\n",
       "       0.51530612, 0.43896976, 0.72303207, 0.18181818, 0.57769653,\n",
       "       0.59310345, 0.33513514, 0.49367089, 0.64065708, 0.08910891,\n",
       "       0.30662021, 0.42592593, 0.01298701, 0.40740741, 0.29063509,\n",
       "       0.05494505, 0.17142857, 0.27272727, 0.26506024, 0.5388601 ,\n",
       "       0.34558824, 0.5158371 , 0.41395349, 0.31944444, 0.35056968,\n",
       "       0.05594406, 0.48201439, 0.08465608, 0.28193833, 0.03125   ,\n",
       "       0.65042174, 0.37931034, 0.01104972, 0.44029851, 0.56047198,\n",
       "       0.36619718, 0.17346939, 0.43333333, 0.54253612, 0.        ,\n",
       "       0.1875    , 0.        , 0.39370079, 0.26506024, 0.32900433,\n",
       "       0.18837675, 0.17910448, 0.7617689 , 0.10344828, 0.28310502,\n",
       "       0.38511327, 0.24752475, 0.47560976, 0.52325581, 0.14117647,\n",
       "       0.52918288, 0.7761807 , 0.29032258, 0.49781659, 0.06451613,\n",
       "       0.06617647, 0.40333333, 0.27692308, 0.36666667, 0.56498237,\n",
       "       0.27972028, 0.65562914, 0.11538462, 0.32653061, 0.49895304,\n",
       "       0.10526316, 0.48167539, 0.35460993, 0.26168224, 0.34482759,\n",
       "       0.24054983, 0.19354839, 0.47029703, 0.43373494, 0.3258427 ,\n",
       "       0.4017094 , 0.24296296, 0.18487395, 0.30065359, 0.68393782,\n",
       "       0.64467005, 0.23316062, 0.03478261, 0.6183844 , 0.35346756])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_tfidf, y_pred,average=None) # giving 100 different F1 Scores belonging to every tag present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_tfidf, y_pred,average=None).shape\n",
    "\n",
    "# we can take the mean of these values for better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35117052776248087"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_score(y_val_tfidf, y_pred,average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still one thing is bothering that is the distribution of tags accross the dataset. As we have already seen, distribution is quite imbalance. Here the weightage is given equally to all the tags. The weightage need to be given according to their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.434766545051494"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_score(y_val_tfidf, y_pred,average=\"micro\")) # Micro gives appropriated weightage based on the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much better performance compare with earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35117052776248087"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_tfidf, y_pred,average=\"macro\") # this is similar to average and give equal weightage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study f1_score sklearn library to learn about more metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, these predictions are arrived at based on the threshold value of 0.5. Try changing the values of threshold can impact the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "y_pred_prob = clf.predict_proba(x_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4594094707520891"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set threshold value\n",
    "t = 0.45\n",
    "\n",
    "# convert to integers\n",
    "y = (y_pred_prob >= t).astype(int)\n",
    "f1_score(y_val_tfidf, y, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see some improvement compare with earlier (0.434766)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buiding Autotagging model is complete with this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How are we going to get tags for new questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must have function which can take a new stackover question as the input and gives out the appropriate tags as output. This entire process is known as Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tags(q):\n",
    "    q = clean_text(q)\n",
    "    q = q.lower()\n",
    "    q = strip_stopwords(q)\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    q_pred = clf.predict(q_vec)\n",
    "    return multilabel_binarizer.inverse_transform(q_pred)\n",
    "\n",
    "# we are following the same steps above in building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 'regression')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give new question\n",
    "new_q = \"Regression line in ggplot doesn't match computed regression Im using R and created a chart using ggplot2. I then create a regression so I can make some predicitions I pass my data frame of to the predict function predict(regression, Measures) I'd expect the predictions to be the same as if I used the regression line on the chart, but they aren't the same. Why would this be the case? Is there a setting in ggplot or is my expectation incorrect?\"\n",
    "\n",
    "# get tags\n",
    "infer_tags(new_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual tags are 'r', 'regression' and 'ggplot'. Our model has predicted 2 tags out of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Autotagging using Deep Learning Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Datasets\n",
    "\n",
    "# Text Cleaning and Preprocessing\n",
    "\n",
    "# Reshape Target Variables (Tags)\n",
    "\n",
    "# Model Building using Keras (steps changes from here, rest all are same like earlier)\n",
    "\n",
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24581</td>\n",
       "      <td>113399</td>\n",
       "      <td>generating a dataset from mean standard deviation n and ci i have the output of a couple of socprog models and i d like to see if the results are statistically significant group a output mean sd c...</td>\n",
       "      <td>[normal-distribution]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23213</td>\n",
       "      <td>9539</td>\n",
       "      <td>how do i go about conducting model diagnostics on wls i m familiar with the diagnostics required for ols however i m in new territory with a model i m fitting to data in r using poisson regression...</td>\n",
       "      <td>[r, modeling]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35167</td>\n",
       "      <td>32810</td>\n",
       "      <td>how can i compare the effect of an pre post intervention between two groups with multiple numeric dependent variables i have collected data from two groups age age in each group i have patients i ...</td>\n",
       "      <td>[anova, statistical-significance, repeated-measures]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46662</td>\n",
       "      <td>148343</td>\n",
       "      <td>interpretation of non significant coefficients my regression uses ols and annual macroeconomics data i find one independent variable x negative and not statistical significant from the theory i ex...</td>\n",
       "      <td>[regression, interpretation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37609</td>\n",
       "      <td>162738</td>\n",
       "      <td>confidence intervals for estimates generated from a non probability sample from what i understand to generate a margin of error to have confidence intervals for a given estimate one needs the stan...</td>\n",
       "      <td>[probability, sampling]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  \\\n",
       "24581  113399   \n",
       "23213    9539   \n",
       "35167   32810   \n",
       "46662  148343   \n",
       "37609  162738   \n",
       "\n",
       "                                                                                                                                                                                                          Text  \\\n",
       "24581  generating a dataset from mean standard deviation n and ci i have the output of a couple of socprog models and i d like to see if the results are statistically significant group a output mean sd c...   \n",
       "23213  how do i go about conducting model diagnostics on wls i m familiar with the diagnostics required for ols however i m in new territory with a model i m fitting to data in r using poisson regression...   \n",
       "35167  how can i compare the effect of an pre post intervention between two groups with multiple numeric dependent variables i have collected data from two groups age age in each group i have patients i ...   \n",
       "46662  interpretation of non significant coefficients my regression uses ols and annual macroeconomics data i find one independent variable x negative and not statistical significant from the theory i ex...   \n",
       "37609  confidence intervals for estimates generated from a non probability sample from what i understand to generate a margin of error to have confidence intervals for a given estimate one needs the stan...   \n",
       "\n",
       "                                                       Tags  \n",
       "24581                                 [normal-distribution]  \n",
       "23213                                         [r, modeling]  \n",
       "35167  [anova, statistical-significance, repeated-measures]  \n",
       "46662                          [regression, interpretation]  \n",
       "37609                               [probability, sampling]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions[['Id','Text','Tags']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step: Representing text samples by integers. Every unique word or term need to be represented by integer\n",
    "\n",
    "\n",
    "This is important because our model accepts only numbers\n",
    "\n",
    "\n",
    "E.g. S1: \"radiation can impact astronauts' memory\" --> array = [1,2,3,4,5]\n",
    "\n",
    "     S2: \"how does memory work\" ---> array = [6,7,5,8]\n",
    "     \n",
    "     Each word is represented by unique integer. Nice representation is available in notes, please refer.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the above line first and then running pandas not giving any error, but the viceversa is throwing error (AttributeError: type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\chalampp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\chalampp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from h5py) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\chalampp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from h5py) (1.16.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below link, it mentioned that downgrading the h5py to 2.9 resolved the issue. Our version 2.10.0, see above\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/issues/36162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\chalampp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\chalampp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from h5py) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\chalampp\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from h5py) (1.16.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_questions['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81956"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique words count\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique words count\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df_questions['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the two cultures statistics vs machine learning last year i read a blog post from brendan o connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to this simon blomberg from r s fortunes package to paraphrase provocatively machine learning is statistics minus any checking of models and assumptions brian d ripley about the difference between machine learning and statistics user vienna may season s greetings andrew gelman in that case maybe we should get rid of checking of models and assumptions more often then maybe we d be able to solve some of the problems that the machine learning people can solve but we can t there was also the statistical modeling the two cultures paper by leo breiman in which argued that statisticians rely too heavily on data modeling and that machine learning techniques are making progress by instead relying on the predictive accuracy of models has the statistics field changed over the last decade in response to these critiques do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines \n",
      "\n",
      "[1, 59, 9730, 255, 315, 482, 256, 577, 277, 2, 333, 3, 2733, 374, 36, 24159, 665, 20232, 9413, 255, 315, 482, 256, 12412, 11, 2734, 60, 5, 1, 441, 74, 1, 59, 2265, 3603, 3384, 5009, 18773, 4, 12, 6933, 50267, 36, 45, 34, 38034, 250, 4, 14051, 50268, 482, 256, 6, 255, 2548, 68, 1771, 5, 132, 7, 691, 9140, 70, 9568, 89, 1, 167, 74, 482, 256, 7, 255, 600, 21877, 264, 1580, 34, 10105, 3603, 3384, 8, 11, 143, 580, 54, 83, 103, 3655, 5, 1771, 5, 132, 7, 691, 92, 726, 81, 580, 54, 70, 19, 375, 4, 588, 60, 5, 1, 597, 11, 1, 482, 256, 320, 28, 588, 31, 54, 28, 17, 43, 97, 105, 1, 235, 772, 1, 59, 9730, 433, 55, 16689, 7210, 8, 47, 6113, 11, 3184, 3258, 427, 3024, 21, 13, 772, 7, 11, 482, 256, 865, 18, 912, 3570, 55, 424, 6334, 21, 1, 785, 406, 5, 132, 100, 1, 255, 1056, 1670, 186, 1, 577, 8085, 8, 234, 4, 94, 15913, 48, 1, 59, 9730, 330, 1223, 33, 100, 255, 6392, 4, 16690, 482, 256, 865, 164, 25, 478, 886, 7, 855, 270, 2100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "print(df_questions['Text'][i], '\\n'), print(sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
