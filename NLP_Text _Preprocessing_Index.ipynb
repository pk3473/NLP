{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Red'>\n",
    "    \n",
    "### Variables are added randomly pick from multiple notebooks. Hence only refer code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Ngrams are very useful in Text Classification\n",
    "    2. Normalization (Stemming & Lemmatization) helps in reducing data dimensionality, text cleaning\n",
    "    3. Constituency Grammar: Any sentence can be organized into 3 constituents (<Subject>,<Context>,<Object>)\n",
    "    4. Dependecny Grammar can be represented in the form of Triplet (Governer, Relation, Dependent)\n",
    "        \n",
    "        (Governer,Relation,Dependent) ==> <Analyticsvidhya> <is> <the largest community of Datascientists>\n",
    "        \n",
    "        Use Cases:\n",
    "        1. Named Entity Recognition\n",
    "        2. Question Answering System\n",
    "        3. Co reference Resolution\n",
    "        4. Text Summarization\n",
    "        5. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"It is raining heavily today and I am not sure if I will be able to travel.\n",
    "Can we postpone our meeting. Hope it is fine with you :) I am sending the new meeting invite on \n",
    "<a href= \"www.example.com\"> this link </a> \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function to get the frequency of words present in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_freq(text):\n",
    "    #Will store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    #Loop over all the tweets and extract words into word_list\n",
    "    for tw_words in text.split():\n",
    "        word_list.extend(tw_words)\n",
    "\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "\n",
    "    #Print top 20 words\n",
    "    word_freq[:20]\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "gen_freq(dataset.text.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "    1. 'dataset' is the name of the dataframe\n",
    "    2. 'text' is the column in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA using Word Clouds\n",
    "\n",
    "    Word Cloud is useful to understand the context of text data using Top 100-200 Words visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Generate word cloud\n",
    "wc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "    word_freq = gen_freq(dataset.text.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Cleaning\n",
    "\n",
    "\n",
    "Text Noise: Unwanted Information in Text Data\n",
    "\n",
    "- Stopwords, URLs, hashtags, punctuations, numbers\n",
    "- Slangs : words which are not present in the dictionaries\n",
    "- Spelling and Grammar errors\n",
    "- Keyword Variations, Apostrophe Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove RT\n",
    "    text = re.sub(r'RT', '', text)\n",
    "    \n",
    "    #Fix &\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    \n",
    "    #Remove punctuations\n",
    "    text = re.sub(r'[?!.;:,#@-]', '', text)\n",
    "\n",
    "    #Convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Removing Punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is raining heavily today and I am not sure if I will be able to travel\\nCan we postpone our meeting Hope it is fine with you  I am sending the new meeting invite on \\na href wwwexamplecom this link a '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punc = string.punctuation\n",
    "\n",
    "text = \"\".join(char for char in text if char not in punc)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Replacing special characters like \\n with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is raining heavily today and I am not sure if I will be able to travel Can we postpone our meeting Hope it is fine with you  I am sending the new meeting invite on  a href wwwexamplecom this link a '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace(\"\\n\",\" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Getting rid of spaces present in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It be rain heavily today and I be not sure if I will be able to travel Can we postpone our meet Hope it be fine with you I be send the new meet invite on a href wwwexamplecom this link a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text_cleaned1.strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stops Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "\n",
    "text = dataset.text.apply(lambda x: clean_text(x))\n",
    "word_freq = gen_freq(text.str)*100\n",
    "word_freq = word_freq.drop(labels=STOPWORDS, errors='ignore')\n",
    "\n",
    "#Generate word cloud\n",
    "wc = WordCloud(width=450, height=330, max_words=200, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(12, 14))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It raining heavily today I sure I able travel Can postpone meeting Hope fine I sending new meeting invite href wwwexamplecom link'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "text_cleaned = \"\"\n",
    "for word in text.split():\n",
    "    if word in stop:\n",
    "        pass\n",
    "    else:\n",
    "        text_cleaned += \" \"\n",
    "        text_cleaned += word \n",
    "\n",
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi John, How are you doing?',\n",
       " 'I will be travelling your city.',\n",
       " 'Lets catchup.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Hi John, How are you doing? I will be travelling your city. Lets catchup.\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'John',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'travelling',\n",
       " 'your',\n",
       " 'city',\n",
       " '.',\n",
       " 'Lets',\n",
       " 'catchup',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "run\n",
      "increas\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('playing'))\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "print(lemm.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemm.lemmatize(\"running\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'NNP'),\n",
       " ('John', 'NNP'),\n",
       " (',', ','),\n",
       " ('How', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('?', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('travelling', 'VBG'),\n",
       " ('your', 'PRP$'),\n",
       " ('city', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Lets', 'VBZ'),\n",
       " ('catchup', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "text =  \"Hi John, How are you doing? I will be travelling your city. Lets catchup.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It is raining heavily today and I am not sure if I will be able to travel Can we postpone our meeting Hope it is fine with you I am sending the new meeting invite on a href wwwexamplecom this link a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "text_cleaned1 = \"\"\n",
    "for word in text.split():\n",
    "    word = lemm.lemmatize(word)\n",
    "    text_cleaned1 += \" \"\n",
    "    text_cleaned1 +=  word\n",
    "    \n",
    "text_cleaned1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It be rain heavily today and I be not sure if I will be able to travel Can we postpone our meet Hope it be fine with you I be send the new meet invite on a href wwwexamplecom this link a'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaned1 = \"\"\n",
    "for word in text.split():\n",
    "    word = lemm.lemmatize(word, pos =\"v\") \n",
    "    text_cleaned1 += \" \"\n",
    "    text_cleaned1 +=  word\n",
    "    \n",
    "text_cleaned1\n",
    "\n",
    "# Raining is converted to rain after defiing POS\n",
    "# meeting is converted to meet, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Extracting Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has also integration with Wordnet which is a very comprehension vocabulary of all the possbile words. We can apply getting synonyms ad antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synonyms\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer.n.01'), Synset('calculator.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Extracting n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000002143748F0C8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'I love to play football'\n",
    "\n",
    "n = 2\n",
    "\n",
    "ngrams(word_tokenize(sentence),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love')\n",
      "('love', 'to')\n",
      "('to', 'play')\n",
      "('play', 'football')\n"
     ]
    }
   ],
   "source": [
    "for i in ngrams(word_tokenize(sentence),n):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Extracting the Index for word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rain': 1,\n",
       " 'again': 2,\n",
       " 'johny': 3,\n",
       " 'wants': 4,\n",
       " 'to': 5,\n",
       " 'play': 6,\n",
       " 'go': 7,\n",
       " 'away': 8,\n",
       " 'come': 9,\n",
       " 'another': 10,\n",
       " 'day': 11,\n",
       " 'little': 12,\n",
       " 'in': 13,\n",
       " 'the': 14}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "data = \"rain rain go away come again another day little johny wants to play johny wants to play again in the rain\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data]) # This data should be list of sentences. In this case, we have only one sentence\n",
    "                               # Due to this every will have an index\n",
    "    \n",
    "tokens = tokenizer.word_index\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Blue'>\n",
    "\n",
    "## Preprocessing Techniques Using SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps you build applications that process and \"understand\" large volumes of text. Its applications are wide in NLP\n",
    "\n",
    "- Tokenisation (Word, Sentence)\n",
    "- Different Properties of Tokens \n",
    "    - Punctuation Removal \n",
    "    - Stopword Removal \n",
    "    - Tokenization\n",
    "    - Word Vector Notations\n",
    "    - Document Similarity etc.\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
